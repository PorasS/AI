{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CNNminist.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMWGfkAmjbzA2OfNw/Cu0Om",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PorasS/AI/blob/master/CNNminist.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MH7_TGxMj1m5"
      },
      "source": [
        "**Keras neural network for digits and fashion minist**\n",
        "https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_06_2_cnn.ipynb\n",
        "---\n",
        "**Computer Vision**\n",
        "\n",
        "This class will focus on computer vision. There are some important differences and similarities with previous neural networks.\n",
        "\n",
        "We will usually use classification, though regression is still an option.\n",
        ">The input to the neural network is now 3D (height, width, color)\n",
        "\n",
        ">Data are not transformed, no z-scores or dummy variables.\n",
        "Processing time is much longer.\n",
        "\n",
        ">We now have different layer times: dense layers (just like before), convolution layers and max pooling layers.\n",
        "\n",
        ">Data will no longer arrive as CSV files. TensorFlow provides some utilities for going directly from image to the input for a neural network.\n",
        "\n",
        "**Computer Vision Data Sets**\n",
        "\n",
        "There are many data sets for computer vision. Two of the most popular are the MNIST digits data set and the CIFAR image data sets.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lncb7IHmZhM"
      },
      "source": [
        "**Other Resources**\n",
        "\n",
        "Imagenet:Large Scale Visual Recognition Challenge 2014\n",
        " - Andrej Karpathy - PhD student/instructor at Stanford.\n",
        "- CS231n Convolutional Neural Networks for Visual Recognition - - - Stanford course on computer vision/CNN's.\n",
        " -CS231n - GitHub\n",
        "- ConvNetJS - JavaScript library for deep learning.\n",
        "\n",
        "\n",
        "Notes:\n",
        ">CNNs utilize overlapping fields of input to simulate features of biological eyes.\n",
        "\n",
        "\n",
        "Convolution Layers\n",
        "The first layer that we will examine is the convolutional layer. We will begin by looking at the hyper-parameters that you must specify for a convolutional layer in most neural network frameworks that support the CNN:\n",
        "\n",
        "Number of filters\n",
        "Filter Size\n",
        "Stride\n",
        "Padding\n",
        "Activation Function/Non-Linearity\n",
        "The primary purpose for a convolutional layer is to detect features such as edges, lines, blobs of color, and other visual elements. The filters can detect these features. The more filters that we give to a convolutional layer, the more features it can detect.\n",
        "\n",
        "A filter is a square-shaped object that scans over the image. A grid can represent the individual pixels of a grid. You can think of the convolutional layer as a smaller grid that sweeps left to right over each row of the image. There is also a hyper parameter that specifies both the width and height of the square-shaped filter. The following figure shows this configuration in which you see the six convolutional filters sweeping over the image grid:\n",
        "\n",
        "A convolutional layer has weights between it and the previous layer or image grid. Each pixel on each convolutional layer is a weight. Therefore, the number of weights between a convolutional layer and its predecessor layer or image field is the following:\n",
        "\n",
        "[FilterSize] * [FilterSize] * [# of Filters]\n",
        "For example, if the filter size were 5 (5x5) for 10 filters, there would be 250 weights.\n",
        "\n",
        "You need to understand how the convolutional filters sweep across the previous layerâ€™s output or image grid. Figure 6.CNN illustrates the sweep:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z8vY8Jiajfxt",
        "outputId": "221b883e-7db5-4293-9beb-3ff07d243800",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import tensorflow.keras\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
        "print(\"Shape of x_train: {}\".format(x_train.shape))\n",
        "print(\"Shape of y_train: {}\".format(y_train.shape))\n",
        "print()\n",
        "print(\"Shape of x_test: {}\".format(x_test.shape))\n",
        "print(\"Shape of y_test: {}\".format(y_test.shape))\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "Shape of x_train: (60000, 28, 28)\n",
            "Shape of y_train: (60000,)\n",
            "\n",
            "Shape of x_test: (10000, 28, 28)\n",
            "Shape of y_test: (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}