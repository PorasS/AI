{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "kerasIntro.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPo2p6xXbLDk/xiQqmPwL1f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PorasS/AI/blob/master/kerasIntro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ckchTTebGurl"
      },
      "source": [
        "**Introduction to tensorflow and keras**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c5sFNAYTHRcX",
        "outputId": "63dcb9d5-e082-471d-d640-baf951233467",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow version: 2.3.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FbOHqUSlLTxb"
      },
      "source": [
        "**Using tensorflow directly**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txx13U-tLYXi",
        "outputId": "cf823380-39f0-49d0-a23f-1e849e01e6c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 537
        }
      },
      "source": [
        "# Import libraries for simulation\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "# Imports for visualization\n",
        "import PIL.Image\n",
        "from io import BytesIO\n",
        "from IPython.display import Image, display\n",
        "\n",
        "def DisplayFractal(a, fmt='jpeg'):\n",
        "  \"\"\"Display an array of iteration counts as a\n",
        "     colorful picture of a fractal.\"\"\"\n",
        "  a_cyclic = (6.28*a/20.0).reshape(list(a.shape)+[1])\n",
        "  img = np.concatenate([10+20*np.cos(a_cyclic),\n",
        "                        30+50*np.sin(a_cyclic),\n",
        "                        155-80*np.cos(a_cyclic)], 2)\n",
        "  img[a==a.max()] = 0\n",
        "  a = img\n",
        "  a = np.uint8(np.clip(a, 0, 255))\n",
        "  f = BytesIO()\n",
        "  PIL.Image.fromarray(a).save(f, fmt)\n",
        "  display(Image(data=f.getvalue()))\n",
        "\n",
        "# Use NumPy to create a 2D array of complex numbers\n",
        "\n",
        "Y, X = np.mgrid[-1.3:1.3:0.005, -2:1:0.005]\n",
        "Z = X+1j*Y\n",
        "\n",
        "xs = tf.constant(Z.astype(np.complex64))\n",
        "zs = tf.Variable(xs)\n",
        "ns = tf.Variable(tf.zeros_like(xs, tf.float32))\n",
        "\n",
        "\n",
        "\n",
        "# Operation to update the zs and the iteration count.\n",
        "#\n",
        "# Note: We keep computing zs after they diverge! This\n",
        "#       is very wasteful! There are better, if a little\n",
        "#       less simple, ways to do this.\n",
        "#\n",
        "for i in range(200):\n",
        "    # Compute the new values of z: z^2 + x\n",
        "    zs_ = zs*zs + xs\n",
        "\n",
        "    # Have we diverged with this new value?\n",
        "    not_diverged = tf.abs(zs_) < 4\n",
        "\n",
        "    zs.assign(zs_),\n",
        "    ns.assign_add(tf.cast(not_diverged, tf.float32))\n",
        "    \n",
        "DisplayFractal(ns.numpy())\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/jpeg": "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\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_G7IzzrSYww",
        "outputId": "3ae5446e-a82c-4794-972b-4c8ad886ffe1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#Matrix multiplication in tensorflow\n",
        "import tensorflow as tf\n",
        "\n",
        "#3x3\n",
        "matrix1= tf.constant([[3.,3.]])\n",
        "\n",
        "#2x1 matrix\n",
        "matrix2= tf.constant([[2.],[2.]])\n",
        "\n",
        "#matrix multiplication\n",
        "product = tf.matmul(matrix1,matrix2)\n",
        "\n",
        "print(product)\n",
        "print(float(product))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([[12.]], shape=(1, 1), dtype=float32)\n",
            "12.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9hjzGFaVRVB",
        "outputId": "7bebb379-bce1-45f4-e37f-39ba27f149fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "x = tf.Variable([1.0,2.0])\n",
        "a = tf.constant([3.0,3.0])\n",
        "\n",
        "sub = tf.subtract(x,a)\n",
        "print(sub)\n",
        "print(sub.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([-2. -1.], shape=(2,), dtype=float32)\n",
            "[-2. -1.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCaHvib-V5DG",
        "outputId": "f82d00bc-6ec2-4060-e931-eb681f46163d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "x.assign([4.0,6.0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Variable 'UnreadVariable' shape=(2,) dtype=float32, numpy=array([4., 6.], dtype=float32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTcTv3p4WFCP",
        "outputId": "d4eb541e-aece-4da0-d7f2-c3e4e532d9e3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "sub = tf.subtract(x,a)\n",
        "print(sub)\n",
        "print(sub.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor([1. 3.], shape=(2,), dtype=float32)\n",
            "[1. 3.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-07rqgE8XIPs"
      },
      "source": [
        "**Introduction to Keras**\n",
        "Keras is a layer on top of tensorflow, that makes it much easier to make neural networks.\n",
        "\n",
        "Lets see mpg model with keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P1FKkcOgXLxu",
        "outputId": "df9249bf-6a2b-4bbd-858d-4ce92efb2fbc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io \n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "display(df[0:5])\n",
        "display(df['horsepower'].isnull().values.any())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>mpg</th>\n",
              "      <th>cylinders</th>\n",
              "      <th>displacement</th>\n",
              "      <th>horsepower</th>\n",
              "      <th>weight</th>\n",
              "      <th>acceleration</th>\n",
              "      <th>year</th>\n",
              "      <th>origin</th>\n",
              "      <th>name</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>307.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>3504</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>chevrolet chevelle malibu</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>15.0</td>\n",
              "      <td>8</td>\n",
              "      <td>350.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>3693</td>\n",
              "      <td>11.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>buick skylark 320</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>18.0</td>\n",
              "      <td>8</td>\n",
              "      <td>318.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3436</td>\n",
              "      <td>11.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>plymouth satellite</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>16.0</td>\n",
              "      <td>8</td>\n",
              "      <td>304.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>3433</td>\n",
              "      <td>12.0</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>amc rebel sst</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>17.0</td>\n",
              "      <td>8</td>\n",
              "      <td>302.0</td>\n",
              "      <td>140.0</td>\n",
              "      <td>3449</td>\n",
              "      <td>10.5</td>\n",
              "      <td>70</td>\n",
              "      <td>1</td>\n",
              "      <td>ford torino</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    mpg  cylinders  displacement  ...  year  origin                       name\n",
              "0  18.0          8         307.0  ...    70       1  chevrolet chevelle malibu\n",
              "1  15.0          8         350.0  ...    70       1          buick skylark 320\n",
              "2  18.0          8         318.0  ...    70       1         plymouth satellite\n",
              "3  16.0          8         304.0  ...    70       1              amc rebel sst\n",
              "4  17.0          8         302.0  ...    70       1                ford torino\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWwr_cZF1Nwd",
        "outputId": "905aa8f1-0e3e-4ddd-b6cb-cf554e2c01d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "cars = df['name']\n",
        "\n",
        "#handle missing values\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "#pandas to numpy\n",
        "x = df[['cylinders','displacement','horsepower','weight','acceleration','year','origin']].values\n",
        "y = df['mpg'].values #regression\n",
        "\n",
        "#Build the neural network\n",
        "#x.shape[1] is no of columns we have, x.shape[0] is for no of rows\n",
        "# relu (rectified linear unit)\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) #Hidden 1\n",
        "model.add(Dense(10, activation= 'relu')) #Hidden 2\n",
        "model.add(Dense(1)) #Output\n",
        "model.compile(loss=\"mean_squared_error\", optimizer = \"adam\")\n",
        "\n",
        "#verbose=0, no progress\n",
        "#verbose=1, Display progress bar, does not  work well with jupyter\n",
        "#verbose=2, Summary progress out (use with jupyter is you want to check the loss at each epoch)\n",
        "model.fit(x,y,verbose=2,epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 - 0s - loss: 12621.4834\n",
            "Epoch 2/100\n",
            "13/13 - 0s - loss: 888.4970\n",
            "Epoch 3/100\n",
            "13/13 - 0s - loss: 906.5664\n",
            "Epoch 4/100\n",
            "13/13 - 0s - loss: 541.1721\n",
            "Epoch 5/100\n",
            "13/13 - 0s - loss: 482.3184\n",
            "Epoch 6/100\n",
            "13/13 - 0s - loss: 456.9390\n",
            "Epoch 7/100\n",
            "13/13 - 0s - loss: 441.4828\n",
            "Epoch 8/100\n",
            "13/13 - 0s - loss: 431.7177\n",
            "Epoch 9/100\n",
            "13/13 - 0s - loss: 426.1723\n",
            "Epoch 10/100\n",
            "13/13 - 0s - loss: 419.3374\n",
            "Epoch 11/100\n",
            "13/13 - 0s - loss: 407.8946\n",
            "Epoch 12/100\n",
            "13/13 - 0s - loss: 398.8240\n",
            "Epoch 13/100\n",
            "13/13 - 0s - loss: 388.7419\n",
            "Epoch 14/100\n",
            "13/13 - 0s - loss: 382.3507\n",
            "Epoch 15/100\n",
            "13/13 - 0s - loss: 370.9897\n",
            "Epoch 16/100\n",
            "13/13 - 0s - loss: 362.1509\n",
            "Epoch 17/100\n",
            "13/13 - 0s - loss: 352.9160\n",
            "Epoch 18/100\n",
            "13/13 - 0s - loss: 343.4314\n",
            "Epoch 19/100\n",
            "13/13 - 0s - loss: 334.6024\n",
            "Epoch 20/100\n",
            "13/13 - 0s - loss: 324.3925\n",
            "Epoch 21/100\n",
            "13/13 - 0s - loss: 314.6441\n",
            "Epoch 22/100\n",
            "13/13 - 0s - loss: 306.4918\n",
            "Epoch 23/100\n",
            "13/13 - 0s - loss: 296.7122\n",
            "Epoch 24/100\n",
            "13/13 - 0s - loss: 288.5957\n",
            "Epoch 25/100\n",
            "13/13 - 0s - loss: 278.6617\n",
            "Epoch 26/100\n",
            "13/13 - 0s - loss: 269.2789\n",
            "Epoch 27/100\n",
            "13/13 - 0s - loss: 261.3112\n",
            "Epoch 28/100\n",
            "13/13 - 0s - loss: 252.0208\n",
            "Epoch 29/100\n",
            "13/13 - 0s - loss: 243.0117\n",
            "Epoch 30/100\n",
            "13/13 - 0s - loss: 235.1381\n",
            "Epoch 31/100\n",
            "13/13 - 0s - loss: 226.5033\n",
            "Epoch 32/100\n",
            "13/13 - 0s - loss: 218.2230\n",
            "Epoch 33/100\n",
            "13/13 - 0s - loss: 209.8420\n",
            "Epoch 34/100\n",
            "13/13 - 0s - loss: 202.4074\n",
            "Epoch 35/100\n",
            "13/13 - 0s - loss: 194.1759\n",
            "Epoch 36/100\n",
            "13/13 - 0s - loss: 187.4437\n",
            "Epoch 37/100\n",
            "13/13 - 0s - loss: 179.7762\n",
            "Epoch 38/100\n",
            "13/13 - 0s - loss: 172.9182\n",
            "Epoch 39/100\n",
            "13/13 - 0s - loss: 167.4186\n",
            "Epoch 40/100\n",
            "13/13 - 0s - loss: 159.2833\n",
            "Epoch 41/100\n",
            "13/13 - 0s - loss: 151.9885\n",
            "Epoch 42/100\n",
            "13/13 - 0s - loss: 145.3873\n",
            "Epoch 43/100\n",
            "13/13 - 0s - loss: 139.9484\n",
            "Epoch 44/100\n",
            "13/13 - 0s - loss: 135.0909\n",
            "Epoch 45/100\n",
            "13/13 - 0s - loss: 127.4431\n",
            "Epoch 46/100\n",
            "13/13 - 0s - loss: 121.5370\n",
            "Epoch 47/100\n",
            "13/13 - 0s - loss: 115.6086\n",
            "Epoch 48/100\n",
            "13/13 - 0s - loss: 111.0333\n",
            "Epoch 49/100\n",
            "13/13 - 0s - loss: 105.2524\n",
            "Epoch 50/100\n",
            "13/13 - 0s - loss: 100.3746\n",
            "Epoch 51/100\n",
            "13/13 - 0s - loss: 95.9789\n",
            "Epoch 52/100\n",
            "13/13 - 0s - loss: 92.4429\n",
            "Epoch 53/100\n",
            "13/13 - 0s - loss: 86.8575\n",
            "Epoch 54/100\n",
            "13/13 - 0s - loss: 82.6954\n",
            "Epoch 55/100\n",
            "13/13 - 0s - loss: 79.2876\n",
            "Epoch 56/100\n",
            "13/13 - 0s - loss: 75.8838\n",
            "Epoch 57/100\n",
            "13/13 - 0s - loss: 73.4165\n",
            "Epoch 58/100\n",
            "13/13 - 0s - loss: 68.7718\n",
            "Epoch 59/100\n",
            "13/13 - 0s - loss: 65.7271\n",
            "Epoch 60/100\n",
            "13/13 - 0s - loss: 61.5953\n",
            "Epoch 61/100\n",
            "13/13 - 0s - loss: 58.8773\n",
            "Epoch 62/100\n",
            "13/13 - 0s - loss: 55.7902\n",
            "Epoch 63/100\n",
            "13/13 - 0s - loss: 53.5021\n",
            "Epoch 64/100\n",
            "13/13 - 0s - loss: 50.7084\n",
            "Epoch 65/100\n",
            "13/13 - 0s - loss: 48.3819\n",
            "Epoch 66/100\n",
            "13/13 - 0s - loss: 46.3330\n",
            "Epoch 67/100\n",
            "13/13 - 0s - loss: 44.8623\n",
            "Epoch 68/100\n",
            "13/13 - 0s - loss: 42.4893\n",
            "Epoch 69/100\n",
            "13/13 - 0s - loss: 40.7850\n",
            "Epoch 70/100\n",
            "13/13 - 0s - loss: 39.0792\n",
            "Epoch 71/100\n",
            "13/13 - 0s - loss: 37.7702\n",
            "Epoch 72/100\n",
            "13/13 - 0s - loss: 36.3177\n",
            "Epoch 73/100\n",
            "13/13 - 0s - loss: 34.9455\n",
            "Epoch 74/100\n",
            "13/13 - 0s - loss: 33.7803\n",
            "Epoch 75/100\n",
            "13/13 - 0s - loss: 33.4488\n",
            "Epoch 76/100\n",
            "13/13 - 0s - loss: 31.9663\n",
            "Epoch 77/100\n",
            "13/13 - 0s - loss: 30.7143\n",
            "Epoch 78/100\n",
            "13/13 - 0s - loss: 29.9356\n",
            "Epoch 79/100\n",
            "13/13 - 0s - loss: 29.0375\n",
            "Epoch 80/100\n",
            "13/13 - 0s - loss: 28.5514\n",
            "Epoch 81/100\n",
            "13/13 - 0s - loss: 28.0330\n",
            "Epoch 82/100\n",
            "13/13 - 0s - loss: 27.3074\n",
            "Epoch 83/100\n",
            "13/13 - 0s - loss: 26.6342\n",
            "Epoch 84/100\n",
            "13/13 - 0s - loss: 26.0431\n",
            "Epoch 85/100\n",
            "13/13 - 0s - loss: 25.5873\n",
            "Epoch 86/100\n",
            "13/13 - 0s - loss: 25.2871\n",
            "Epoch 87/100\n",
            "13/13 - 0s - loss: 25.1862\n",
            "Epoch 88/100\n",
            "13/13 - 0s - loss: 25.1371\n",
            "Epoch 89/100\n",
            "13/13 - 0s - loss: 24.4295\n",
            "Epoch 90/100\n",
            "13/13 - 0s - loss: 24.0178\n",
            "Epoch 91/100\n",
            "13/13 - 0s - loss: 24.1627\n",
            "Epoch 92/100\n",
            "13/13 - 0s - loss: 25.3765\n",
            "Epoch 93/100\n",
            "13/13 - 0s - loss: 24.0670\n",
            "Epoch 94/100\n",
            "13/13 - 0s - loss: 22.8247\n",
            "Epoch 95/100\n",
            "13/13 - 0s - loss: 22.5526\n",
            "Epoch 96/100\n",
            "13/13 - 0s - loss: 22.7180\n",
            "Epoch 97/100\n",
            "13/13 - 0s - loss: 22.8620\n",
            "Epoch 98/100\n",
            "13/13 - 0s - loss: 22.4043\n",
            "Epoch 99/100\n",
            "13/13 - 0s - loss: 23.2955\n",
            "Epoch 100/100\n",
            "13/13 - 0s - loss: 23.8130\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f00fbe1f550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NVLnsrDu8G5_",
        "outputId": "ca3681d8-7ced-4a82-8846-acdd6d65e91f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Lets predict with the model\n",
        "pred = model.predict(x)\n",
        "print(\"Shape:{}\".format(pred.shape))\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape:(398, 1)\n",
            "[[14.793416 ]\n",
            " [14.043078 ]\n",
            " [14.940078 ]\n",
            " [16.229836 ]\n",
            " [15.231225 ]\n",
            " [10.655721 ]\n",
            " [10.025864 ]\n",
            " [10.465955 ]\n",
            " [10.657987 ]\n",
            " [11.994918 ]\n",
            " [11.527614 ]\n",
            " [13.171233 ]\n",
            " [ 9.046194 ]\n",
            " [ 9.838915 ]\n",
            " [27.024357 ]\n",
            " [20.651392 ]\n",
            " [20.674726 ]\n",
            " [19.895866 ]\n",
            " [27.350817 ]\n",
            " [25.948933 ]\n",
            " [27.176357 ]\n",
            " [26.41018  ]\n",
            " [27.91358  ]\n",
            " [26.127586 ]\n",
            " [19.982248 ]\n",
            " [17.959496 ]\n",
            " [20.804924 ]\n",
            " [20.19011  ]\n",
            " [21.963388 ]\n",
            " [27.631845 ]\n",
            " [24.109152 ]\n",
            " [26.879337 ]\n",
            " [28.242798 ]\n",
            " [17.968634 ]\n",
            " [20.101656 ]\n",
            " [18.0125   ]\n",
            " [17.230679 ]\n",
            " [19.21358  ]\n",
            " [14.808699 ]\n",
            " [12.027529 ]\n",
            " [14.448573 ]\n",
            " [16.300695 ]\n",
            " [13.804744 ]\n",
            " [12.050089 ]\n",
            " [12.60999  ]\n",
            " [17.204914 ]\n",
            " [24.232075 ]\n",
            " [17.814777 ]\n",
            " [16.793022 ]\n",
            " [24.546362 ]\n",
            " [25.721384 ]\n",
            " [28.79496  ]\n",
            " [26.86935  ]\n",
            " [29.238476 ]\n",
            " [28.989647 ]\n",
            " [26.615303 ]\n",
            " [27.672167 ]\n",
            " [27.69789  ]\n",
            " [27.06999  ]\n",
            " [28.29131  ]\n",
            " [25.830038 ]\n",
            " [25.675928 ]\n",
            " [15.129521 ]\n",
            " [12.429233 ]\n",
            " [16.774574 ]\n",
            " [14.545317 ]\n",
            " [16.769152 ]\n",
            " [12.372105 ]\n",
            " [15.1381235]\n",
            " [15.428854 ]\n",
            " [13.577449 ]\n",
            " [30.056877 ]\n",
            " [17.24165  ]\n",
            " [16.394663 ]\n",
            " [18.168835 ]\n",
            " [16.908033 ]\n",
            " [27.729351 ]\n",
            " [26.358044 ]\n",
            " [27.920893 ]\n",
            " [27.41764  ]\n",
            " [25.610317 ]\n",
            " [29.109484 ]\n",
            " [27.150154 ]\n",
            " [26.383305 ]\n",
            " [28.570417 ]\n",
            " [16.279802 ]\n",
            " [17.05017  ]\n",
            " [14.298019 ]\n",
            " [17.597479 ]\n",
            " [16.498682 ]\n",
            " [12.379468 ]\n",
            " [11.164167 ]\n",
            " [15.288359 ]\n",
            " [17.45594  ]\n",
            " [12.412658 ]\n",
            " [12.160733 ]\n",
            " [14.752062 ]\n",
            " [20.807108 ]\n",
            " [19.38818  ]\n",
            " [19.734747 ]\n",
            " [17.958761 ]\n",
            " [21.706856 ]\n",
            " [27.031315 ]\n",
            " [12.166055 ]\n",
            " [12.68764  ]\n",
            " [15.618587 ]\n",
            " [16.67379  ]\n",
            " [19.301418 ]\n",
            " [29.805779 ]\n",
            " [24.958834 ]\n",
            " [28.655445 ]\n",
            " [29.765467 ]\n",
            " [26.620419 ]\n",
            " [24.461176 ]\n",
            " [28.103361 ]\n",
            " [14.355522 ]\n",
            " [15.3075905]\n",
            " [28.640429 ]\n",
            " [25.85511  ]\n",
            " [26.764233 ]\n",
            " [28.308546 ]\n",
            " [15.760543 ]\n",
            " [27.546833 ]\n",
            " [26.52668  ]\n",
            " [15.656113 ]\n",
            " [22.277983 ]\n",
            " [22.076185 ]\n",
            " [19.988886 ]\n",
            " [19.36678  ]\n",
            " [29.772427 ]\n",
            " [25.992973 ]\n",
            " [30.79597  ]\n",
            " [24.672565 ]\n",
            " [19.638964 ]\n",
            " [19.978569 ]\n",
            " [21.3891   ]\n",
            " [17.96142  ]\n",
            " [15.839768 ]\n",
            " [17.53362  ]\n",
            " [18.941278 ]\n",
            " [19.040815 ]\n",
            " [28.247757 ]\n",
            " [28.027079 ]\n",
            " [27.37065  ]\n",
            " [27.9912   ]\n",
            " [29.149706 ]\n",
            " [26.978577 ]\n",
            " [27.876665 ]\n",
            " [25.68307  ]\n",
            " [27.870773 ]\n",
            " [28.542091 ]\n",
            " [28.218657 ]\n",
            " [20.649912 ]\n",
            " [19.703985 ]\n",
            " [19.272589 ]\n",
            " [18.598059 ]\n",
            " [12.957584 ]\n",
            " [15.474498 ]\n",
            " [18.17759  ]\n",
            " [15.561461 ]\n",
            " [23.280876 ]\n",
            " [20.816767 ]\n",
            " [20.65752  ]\n",
            " [21.982435 ]\n",
            " [20.72227  ]\n",
            " [18.322243 ]\n",
            " [16.270514 ]\n",
            " [28.45888  ]\n",
            " [25.523144 ]\n",
            " [20.277868 ]\n",
            " [25.682442 ]\n",
            " [26.757767 ]\n",
            " [28.310875 ]\n",
            " [28.930038 ]\n",
            " [23.777739 ]\n",
            " [27.227303 ]\n",
            " [20.15973  ]\n",
            " [27.919743 ]\n",
            " [27.969467 ]\n",
            " [27.686918 ]\n",
            " [28.265583 ]\n",
            " [27.741465 ]\n",
            " [28.199984 ]\n",
            " [27.59191  ]\n",
            " [25.627481 ]\n",
            " [28.41174  ]\n",
            " [28.189528 ]\n",
            " [18.026411 ]\n",
            " [17.76336  ]\n",
            " [16.968435 ]\n",
            " [15.590625 ]\n",
            " [21.02811  ]\n",
            " [19.413244 ]\n",
            " [22.12761  ]\n",
            " [20.56646  ]\n",
            " [28.961485 ]\n",
            " [28.631275 ]\n",
            " [27.575926 ]\n",
            " [27.988684 ]\n",
            " [22.061075 ]\n",
            " [20.023087 ]\n",
            " [20.485247 ]\n",
            " [19.247879 ]\n",
            " [26.418493 ]\n",
            " [29.465687 ]\n",
            " [28.8653   ]\n",
            " [23.908384 ]\n",
            " [28.140882 ]\n",
            " [17.678034 ]\n",
            " [30.097837 ]\n",
            " [27.228035 ]\n",
            " [27.555271 ]\n",
            " [17.308876 ]\n",
            " [14.844148 ]\n",
            " [18.057903 ]\n",
            " [17.835205 ]\n",
            " [29.274168 ]\n",
            " [26.829796 ]\n",
            " [29.259398 ]\n",
            " [27.42616  ]\n",
            " [29.651596 ]\n",
            " [18.25248  ]\n",
            " [21.392397 ]\n",
            " [17.931454 ]\n",
            " [18.565083 ]\n",
            " [20.757397 ]\n",
            " [21.84366  ]\n",
            " [22.32925  ]\n",
            " [20.87379  ]\n",
            " [13.74817  ]\n",
            " [16.58411  ]\n",
            " [14.821878 ]\n",
            " [16.328306 ]\n",
            " [27.993526 ]\n",
            " [25.378592 ]\n",
            " [29.821922 ]\n",
            " [26.133314 ]\n",
            " [27.311012 ]\n",
            " [28.229485 ]\n",
            " [28.532133 ]\n",
            " [28.011274 ]\n",
            " [27.080425 ]\n",
            " [28.228718 ]\n",
            " [31.848486 ]\n",
            " [29.231236 ]\n",
            " [26.75119  ]\n",
            " [30.164616 ]\n",
            " [30.617393 ]\n",
            " [28.662287 ]\n",
            " [20.065525 ]\n",
            " [17.476952 ]\n",
            " [18.266937 ]\n",
            " [22.9692   ]\n",
            " [23.87275  ]\n",
            " [22.307747 ]\n",
            " [26.193945 ]\n",
            " [22.318943 ]\n",
            " [21.069788 ]\n",
            " [21.725418 ]\n",
            " [22.676115 ]\n",
            " [23.579836 ]\n",
            " [20.624634 ]\n",
            " [18.49174  ]\n",
            " [24.780628 ]\n",
            " [17.503002 ]\n",
            " [17.856932 ]\n",
            " [27.805563 ]\n",
            " [27.686745 ]\n",
            " [28.846008 ]\n",
            " [27.145145 ]\n",
            " [27.862003 ]\n",
            " [26.64285  ]\n",
            " [26.079376 ]\n",
            " [28.977829 ]\n",
            " [28.62405  ]\n",
            " [27.313288 ]\n",
            " [29.92795  ]\n",
            " [28.732113 ]\n",
            " [28.538853 ]\n",
            " [28.968178 ]\n",
            " [22.426462 ]\n",
            " [23.41509  ]\n",
            " [27.22105  ]\n",
            " [21.7224   ]\n",
            " [22.99217  ]\n",
            " [18.81343  ]\n",
            " [18.207788 ]\n",
            " [15.517062 ]\n",
            " [18.173065 ]\n",
            " [17.491585 ]\n",
            " [16.204424 ]\n",
            " [20.804188 ]\n",
            " [15.592515 ]\n",
            " [28.475988 ]\n",
            " [29.304314 ]\n",
            " [27.995434 ]\n",
            " [27.093294 ]\n",
            " [25.552261 ]\n",
            " [16.141808 ]\n",
            " [29.357332 ]\n",
            " [21.368887 ]\n",
            " [26.649635 ]\n",
            " [27.193552 ]\n",
            " [30.751724 ]\n",
            " [28.57415  ]\n",
            " [26.025421 ]\n",
            " [24.594229 ]\n",
            " [25.19914  ]\n",
            " [25.009428 ]\n",
            " [28.833342 ]\n",
            " [30.274412 ]\n",
            " [28.135832 ]\n",
            " [30.01778  ]\n",
            " [26.4803   ]\n",
            " [27.760195 ]\n",
            " [27.895704 ]\n",
            " [22.720184 ]\n",
            " [29.427658 ]\n",
            " [28.461609 ]\n",
            " [29.031153 ]\n",
            " [29.272537 ]\n",
            " [28.90187  ]\n",
            " [30.58036  ]\n",
            " [26.461287 ]\n",
            " [31.0878   ]\n",
            " [29.92206  ]\n",
            " [30.750841 ]\n",
            " [28.996557 ]\n",
            " [28.065536 ]\n",
            " [28.822725 ]\n",
            " [31.524586 ]\n",
            " [30.0138   ]\n",
            " [28.57342  ]\n",
            " [27.667542 ]\n",
            " [32.21356  ]\n",
            " [28.316729 ]\n",
            " [26.848215 ]\n",
            " [29.402245 ]\n",
            " [27.08324  ]\n",
            " [26.318584 ]\n",
            " [25.803144 ]\n",
            " [25.356213 ]\n",
            " [26.072777 ]\n",
            " [30.33671  ]\n",
            " [29.005922 ]\n",
            " [30.060719 ]\n",
            " [30.178303 ]\n",
            " [31.353834 ]\n",
            " [30.226233 ]\n",
            " [29.993572 ]\n",
            " [27.348946 ]\n",
            " [28.288664 ]\n",
            " [30.014313 ]\n",
            " [28.369156 ]\n",
            " [30.5736   ]\n",
            " [28.947023 ]\n",
            " [29.775585 ]\n",
            " [30.106901 ]\n",
            " [29.575657 ]\n",
            " [29.030926 ]\n",
            " [28.2974   ]\n",
            " [27.327585 ]\n",
            " [29.505188 ]\n",
            " [22.908796 ]\n",
            " [15.861988 ]\n",
            " [23.839577 ]\n",
            " [22.024075 ]\n",
            " [30.57442  ]\n",
            " [30.257885 ]\n",
            " [29.905252 ]\n",
            " [29.21575  ]\n",
            " [27.48704  ]\n",
            " [27.58413  ]\n",
            " [27.999825 ]\n",
            " [28.835686 ]\n",
            " [28.893436 ]\n",
            " [31.042526 ]\n",
            " [30.806108 ]\n",
            " [27.507322 ]\n",
            " [29.309229 ]\n",
            " [29.174786 ]\n",
            " [29.258781 ]\n",
            " [29.707304 ]\n",
            " [29.86065  ]\n",
            " [30.097197 ]\n",
            " [30.28452  ]\n",
            " [26.511063 ]\n",
            " [19.644608 ]\n",
            " [26.09658  ]\n",
            " [22.522766 ]\n",
            " [28.156315 ]\n",
            " [26.3784   ]\n",
            " [27.479094 ]\n",
            " [27.300941 ]\n",
            " [31.269918 ]\n",
            " [25.859413 ]\n",
            " [29.129768 ]\n",
            " [29.717686 ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_jwL8Qg82jl",
        "outputId": "7e84ea62-67ee-46df-b2bc-509f609b9f4c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Measure RMSE error. RMSE is common for regression\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
        "print(f\"Final score (RMSE): {score}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Final score (RMSE): 4.685817623657029\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZQya7Lz_kjV"
      },
      "source": [
        "This means that, on average the prediction were within +/- 5.89 values of the correct value.\n",
        "\n",
        "We can also print out the first 10 cars, with predictions and actual mpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E14Gp8NJ_iDn",
        "outputId": "83e88074-3b78-41fc-fce0-bc7e08371a87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Sample predictions\n",
        "for i in range(10):\n",
        "  print(f\"{i+1}. Car Name: {cars[i]}, MPG: {y[i]}, predicted MPG: {pred[i]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. Car Name: chevrolet chevelle malibu, MPG: 18.0, predicted MPG: [14.793416]\n",
            "2. Car Name: buick skylark 320, MPG: 15.0, predicted MPG: [14.043078]\n",
            "3. Car Name: plymouth satellite, MPG: 18.0, predicted MPG: [14.940078]\n",
            "4. Car Name: amc rebel sst, MPG: 16.0, predicted MPG: [16.229836]\n",
            "5. Car Name: ford torino, MPG: 17.0, predicted MPG: [15.231225]\n",
            "6. Car Name: ford galaxie 500, MPG: 15.0, predicted MPG: [10.655721]\n",
            "7. Car Name: chevrolet impala, MPG: 14.0, predicted MPG: [10.025864]\n",
            "8. Car Name: plymouth fury iii, MPG: 14.0, predicted MPG: [10.465955]\n",
            "9. Car Name: pontiac catalina, MPG: 14.0, predicted MPG: [10.657987]\n",
            "10. Car Name: amc ambassador dpl, MPG: 15.0, predicted MPG: [11.994918]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVLEnzofBcla"
      },
      "source": [
        "**Simple tensorflow classification**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ud_PXwDNGxfc",
        "outputId": "6f443238-a9b3-41cb-dc2e-6ca1e68aada7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/iris.csv\", na_values=['NA', '?'])\n",
        "\n",
        "display(df[0:5])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_l</th>\n",
              "      <th>sepal_w</th>\n",
              "      <th>petal_l</th>\n",
              "      <th>petal_w</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sepal_l  sepal_w  petal_l  petal_w      species\n",
              "0      5.1      3.5      1.4      0.2  Iris-setosa\n",
              "1      4.9      3.0      1.4      0.2  Iris-setosa\n",
              "2      4.7      3.2      1.3      0.2  Iris-setosa\n",
              "3      4.6      3.1      1.5      0.2  Iris-setosa\n",
              "4      5.0      3.6      1.4      0.2  Iris-setosa"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PQKOS3o8H5mu",
        "outputId": "c96aeaa7-e7a6-4412-d54f-2a6f275b0cb4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Convert to numpy calssification\n",
        "x = df[['sepal_l','sepal_w','petal_l','petal_w']].values\n",
        "dummies = pd.get_dummies(df['species']) # Classification\n",
        "species = dummies.columns\n",
        "y = dummies.values\n",
        "\n",
        "\n",
        "# Build Neural Network\n",
        "model = Sequential()\n",
        "model.add(Dense(50, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(25, activation='relu')) # Hidden 2\n",
        "model.add(Dense(y.shape[1],activation='softmax')) # output\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer = 'adam')\n",
        "model.fit(x,y, verbose=2, epochs=100)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "5/5 - 0s - loss: 1.1223\n",
            "Epoch 2/100\n",
            "5/5 - 0s - loss: 1.0566\n",
            "Epoch 3/100\n",
            "5/5 - 0s - loss: 1.0042\n",
            "Epoch 4/100\n",
            "5/5 - 0s - loss: 0.9557\n",
            "Epoch 5/100\n",
            "5/5 - 0s - loss: 0.9146\n",
            "Epoch 6/100\n",
            "5/5 - 0s - loss: 0.8687\n",
            "Epoch 7/100\n",
            "5/5 - 0s - loss: 0.8274\n",
            "Epoch 8/100\n",
            "5/5 - 0s - loss: 0.7895\n",
            "Epoch 9/100\n",
            "5/5 - 0s - loss: 0.7485\n",
            "Epoch 10/100\n",
            "5/5 - 0s - loss: 0.7109\n",
            "Epoch 11/100\n",
            "5/5 - 0s - loss: 0.6739\n",
            "Epoch 12/100\n",
            "5/5 - 0s - loss: 0.6397\n",
            "Epoch 13/100\n",
            "5/5 - 0s - loss: 0.6069\n",
            "Epoch 14/100\n",
            "5/5 - 0s - loss: 0.5780\n",
            "Epoch 15/100\n",
            "5/5 - 0s - loss: 0.5522\n",
            "Epoch 16/100\n",
            "5/5 - 0s - loss: 0.5286\n",
            "Epoch 17/100\n",
            "5/5 - 0s - loss: 0.5072\n",
            "Epoch 18/100\n",
            "5/5 - 0s - loss: 0.4889\n",
            "Epoch 19/100\n",
            "5/5 - 0s - loss: 0.4718\n",
            "Epoch 20/100\n",
            "5/5 - 0s - loss: 0.4599\n",
            "Epoch 21/100\n",
            "5/5 - 0s - loss: 0.4434\n",
            "Epoch 22/100\n",
            "5/5 - 0s - loss: 0.4293\n",
            "Epoch 23/100\n",
            "5/5 - 0s - loss: 0.4169\n",
            "Epoch 24/100\n",
            "5/5 - 0s - loss: 0.4056\n",
            "Epoch 25/100\n",
            "5/5 - 0s - loss: 0.3966\n",
            "Epoch 26/100\n",
            "5/5 - 0s - loss: 0.3848\n",
            "Epoch 27/100\n",
            "5/5 - 0s - loss: 0.3763\n",
            "Epoch 28/100\n",
            "5/5 - 0s - loss: 0.3645\n",
            "Epoch 29/100\n",
            "5/5 - 0s - loss: 0.3565\n",
            "Epoch 30/100\n",
            "5/5 - 0s - loss: 0.3468\n",
            "Epoch 31/100\n",
            "5/5 - 0s - loss: 0.3375\n",
            "Epoch 32/100\n",
            "5/5 - 0s - loss: 0.3299\n",
            "Epoch 33/100\n",
            "5/5 - 0s - loss: 0.3219\n",
            "Epoch 34/100\n",
            "5/5 - 0s - loss: 0.3118\n",
            "Epoch 35/100\n",
            "5/5 - 0s - loss: 0.3039\n",
            "Epoch 36/100\n",
            "5/5 - 0s - loss: 0.2966\n",
            "Epoch 37/100\n",
            "5/5 - 0s - loss: 0.2911\n",
            "Epoch 38/100\n",
            "5/5 - 0s - loss: 0.2814\n",
            "Epoch 39/100\n",
            "5/5 - 0s - loss: 0.2750\n",
            "Epoch 40/100\n",
            "5/5 - 0s - loss: 0.2677\n",
            "Epoch 41/100\n",
            "5/5 - 0s - loss: 0.2599\n",
            "Epoch 42/100\n",
            "5/5 - 0s - loss: 0.2553\n",
            "Epoch 43/100\n",
            "5/5 - 0s - loss: 0.2463\n",
            "Epoch 44/100\n",
            "5/5 - 0s - loss: 0.2402\n",
            "Epoch 45/100\n",
            "5/5 - 0s - loss: 0.2389\n",
            "Epoch 46/100\n",
            "5/5 - 0s - loss: 0.2286\n",
            "Epoch 47/100\n",
            "5/5 - 0s - loss: 0.2241\n",
            "Epoch 48/100\n",
            "5/5 - 0s - loss: 0.2146\n",
            "Epoch 49/100\n",
            "5/5 - 0s - loss: 0.2089\n",
            "Epoch 50/100\n",
            "5/5 - 0s - loss: 0.2016\n",
            "Epoch 51/100\n",
            "5/5 - 0s - loss: 0.1975\n",
            "Epoch 52/100\n",
            "5/5 - 0s - loss: 0.1926\n",
            "Epoch 53/100\n",
            "5/5 - 0s - loss: 0.1855\n",
            "Epoch 54/100\n",
            "5/5 - 0s - loss: 0.1798\n",
            "Epoch 55/100\n",
            "5/5 - 0s - loss: 0.1768\n",
            "Epoch 56/100\n",
            "5/5 - 0s - loss: 0.1726\n",
            "Epoch 57/100\n",
            "5/5 - 0s - loss: 0.1670\n",
            "Epoch 58/100\n",
            "5/5 - 0s - loss: 0.1628\n",
            "Epoch 59/100\n",
            "5/5 - 0s - loss: 0.1602\n",
            "Epoch 60/100\n",
            "5/5 - 0s - loss: 0.1546\n",
            "Epoch 61/100\n",
            "5/5 - 0s - loss: 0.1520\n",
            "Epoch 62/100\n",
            "5/5 - 0s - loss: 0.1493\n",
            "Epoch 63/100\n",
            "5/5 - 0s - loss: 0.1462\n",
            "Epoch 64/100\n",
            "5/5 - 0s - loss: 0.1419\n",
            "Epoch 65/100\n",
            "5/5 - 0s - loss: 0.1380\n",
            "Epoch 66/100\n",
            "5/5 - 0s - loss: 0.1393\n",
            "Epoch 67/100\n",
            "5/5 - 0s - loss: 0.1355\n",
            "Epoch 68/100\n",
            "5/5 - 0s - loss: 0.1346\n",
            "Epoch 69/100\n",
            "5/5 - 0s - loss: 0.1269\n",
            "Epoch 70/100\n",
            "5/5 - 0s - loss: 0.1275\n",
            "Epoch 71/100\n",
            "5/5 - 0s - loss: 0.1251\n",
            "Epoch 72/100\n",
            "5/5 - 0s - loss: 0.1226\n",
            "Epoch 73/100\n",
            "5/5 - 0s - loss: 0.1218\n",
            "Epoch 74/100\n",
            "5/5 - 0s - loss: 0.1188\n",
            "Epoch 75/100\n",
            "5/5 - 0s - loss: 0.1157\n",
            "Epoch 76/100\n",
            "5/5 - 0s - loss: 0.1145\n",
            "Epoch 77/100\n",
            "5/5 - 0s - loss: 0.1128\n",
            "Epoch 78/100\n",
            "5/5 - 0s - loss: 0.1116\n",
            "Epoch 79/100\n",
            "5/5 - 0s - loss: 0.1150\n",
            "Epoch 80/100\n",
            "5/5 - 0s - loss: 0.1064\n",
            "Epoch 81/100\n",
            "5/5 - 0s - loss: 0.1090\n",
            "Epoch 82/100\n",
            "5/5 - 0s - loss: 0.1042\n",
            "Epoch 83/100\n",
            "5/5 - 0s - loss: 0.1067\n",
            "Epoch 84/100\n",
            "5/5 - 0s - loss: 0.1035\n",
            "Epoch 85/100\n",
            "5/5 - 0s - loss: 0.1035\n",
            "Epoch 86/100\n",
            "5/5 - 0s - loss: 0.1001\n",
            "Epoch 87/100\n",
            "5/5 - 0s - loss: 0.0996\n",
            "Epoch 88/100\n",
            "5/5 - 0s - loss: 0.0986\n",
            "Epoch 89/100\n",
            "5/5 - 0s - loss: 0.0973\n",
            "Epoch 90/100\n",
            "5/5 - 0s - loss: 0.0978\n",
            "Epoch 91/100\n",
            "5/5 - 0s - loss: 0.0965\n",
            "Epoch 92/100\n",
            "5/5 - 0s - loss: 0.0961\n",
            "Epoch 93/100\n",
            "5/5 - 0s - loss: 0.0935\n",
            "Epoch 94/100\n",
            "5/5 - 0s - loss: 0.0915\n",
            "Epoch 95/100\n",
            "5/5 - 0s - loss: 0.0917\n",
            "Epoch 96/100\n",
            "5/5 - 0s - loss: 0.0919\n",
            "Epoch 97/100\n",
            "5/5 - 0s - loss: 0.0892\n",
            "Epoch 98/100\n",
            "5/5 - 0s - loss: 0.0889\n",
            "Epoch 99/100\n",
            "5/5 - 0s - loss: 0.0904\n",
            "Epoch 100/100\n",
            "5/5 - 0s - loss: 0.0894\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f01028b4710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZzP-cO9KlpA",
        "outputId": "07467a12-bd21-4e68-9320-ee6cb26c52d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# print out the no of species found\n",
        "print(species)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Iris-setosa', 'Iris-versicolor', 'Iris-virginica'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BAc46l2KuRa",
        "outputId": "d0e39482-51aa-45c0-a0fd-ca0a44eed5d6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "pred =  model.predict(x)\n",
        "print(\"Shape: {pred.shape}\")\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Shape: {pred.shape}\n",
            "[[9.99485016e-01 5.14927611e-04 6.32588160e-09]\n",
            " [9.97497618e-01 2.50228192e-03 7.43167448e-08]\n",
            " [9.99032021e-01 9.67948290e-04 2.65880047e-08]\n",
            " [9.97573435e-01 2.42649950e-03 9.95063374e-08]\n",
            " [9.99626279e-01 3.73722403e-04 4.58443239e-09]\n",
            " [9.99481022e-01 5.19011461e-04 4.71898032e-09]\n",
            " [9.99186814e-01 8.13152001e-04 2.52665870e-08]\n",
            " [9.99058902e-01 9.41151928e-04 1.60023195e-08]\n",
            " [9.96645033e-01 3.35468119e-03 2.11148830e-07]\n",
            " [9.97954130e-01 2.04584841e-03 4.82400679e-08]\n",
            " [9.99628544e-01 3.71478061e-04 2.62276556e-09]\n",
            " [9.98730600e-01 1.26935053e-03 2.98678664e-08]\n",
            " [9.97940600e-01 2.05942267e-03 5.79382196e-08]\n",
            " [9.99158621e-01 8.41423171e-04 3.54470302e-08]\n",
            " [9.99934316e-01 6.56901466e-05 1.55313734e-10]\n",
            " [9.99938488e-01 6.15158278e-05 1.76626130e-10]\n",
            " [9.99834538e-01 1.65493489e-04 1.12281051e-09]\n",
            " [9.99381304e-01 6.18711696e-04 9.08659903e-09]\n",
            " [9.99395132e-01 6.04913745e-04 3.59096242e-09]\n",
            " [9.99681115e-01 3.18900042e-04 3.44870843e-09]\n",
            " [9.98203397e-01 1.79654919e-03 2.15550831e-08]\n",
            " [9.99476135e-01 5.23837341e-04 7.68268293e-09]\n",
            " [9.99859333e-01 1.40674616e-04 2.16242957e-09]\n",
            " [9.95290399e-01 4.70954087e-03 1.68539430e-07]\n",
            " [9.96656537e-01 3.34336888e-03 1.02237237e-07]\n",
            " [9.95238185e-01 4.76173498e-03 1.46516626e-07]\n",
            " [9.98125494e-01 1.87450123e-03 4.91551333e-08]\n",
            " [9.99311090e-01 6.88968459e-04 8.03784062e-09]\n",
            " [9.99290586e-01 7.09445914e-04 8.72834249e-09]\n",
            " [9.97589350e-01 2.41050706e-03 8.28502351e-08]\n",
            " [9.96649086e-01 3.35083227e-03 1.15429586e-07]\n",
            " [9.98593032e-01 1.40695786e-03 2.07712176e-08]\n",
            " [9.99912858e-01 8.71094599e-05 3.98591160e-10]\n",
            " [9.99940515e-01 5.95253587e-05 1.83697182e-10]\n",
            " [9.97529089e-01 2.47083697e-03 6.96802331e-08]\n",
            " [9.99247313e-01 7.52706663e-04 1.32322437e-08]\n",
            " [9.99596417e-01 4.03542537e-04 2.80091261e-09]\n",
            " [9.99691486e-01 3.08584538e-04 3.58682417e-09]\n",
            " [9.98220861e-01 1.77906710e-03 9.14646989e-08]\n",
            " [9.99053180e-01 9.46859655e-04 1.42104604e-08]\n",
            " [9.99533772e-01 4.66202560e-04 7.21457338e-09]\n",
            " [9.77687895e-01 2.23089438e-02 3.09621760e-06]\n",
            " [9.99054611e-01 9.45402891e-04 3.77453624e-08]\n",
            " [9.97922361e-01 2.07755808e-03 7.09329058e-08]\n",
            " [9.98625159e-01 1.37485692e-03 2.58268802e-08]\n",
            " [9.96996284e-01 3.00362590e-03 1.20873011e-07]\n",
            " [9.99646306e-01 3.53638956e-04 3.44235818e-09]\n",
            " [9.98713732e-01 1.28626567e-03 4.30847855e-08]\n",
            " [9.99631405e-01 3.68569075e-04 2.94769875e-09]\n",
            " [9.99045312e-01 9.54611576e-04 1.70956973e-08]\n",
            " [7.15460512e-04 9.87857819e-01 1.14267599e-02]\n",
            " [1.18203298e-03 9.69467103e-01 2.93508619e-02]\n",
            " [6.67616434e-04 9.27537024e-01 7.17953146e-02]\n",
            " [2.21356377e-03 7.81621993e-01 2.16164380e-01]\n",
            " [9.62887134e-04 8.65483940e-01 1.33553103e-01]\n",
            " [1.47071423e-03 7.95175910e-01 2.03353345e-01]\n",
            " [1.08697254e-03 9.08662617e-01 9.02503654e-02]\n",
            " [7.46214669e-03 9.82074142e-01 1.04636941e-02]\n",
            " [9.43724415e-04 9.65325594e-01 3.37306745e-02]\n",
            " [3.19259218e-03 8.78517807e-01 1.18289612e-01]\n",
            " [4.39939415e-03 9.25463021e-01 7.01375827e-02]\n",
            " [1.89233816e-03 9.47718799e-01 5.03888354e-02]\n",
            " [1.83035561e-03 9.66731071e-01 3.14384699e-02]\n",
            " [1.03843550e-03 7.88978040e-01 2.09983513e-01]\n",
            " [6.57875882e-03 9.86814857e-01 6.60638511e-03]\n",
            " [1.02613110e-03 9.90077138e-01 8.89666192e-03]\n",
            " [1.48559478e-03 7.19559908e-01 2.78954506e-01]\n",
            " [1.94711785e-03 9.84902561e-01 1.31502934e-02]\n",
            " [5.48885379e-04 3.71255100e-01 6.28196001e-01]\n",
            " [2.52889306e-03 9.69878376e-01 2.75927410e-02]\n",
            " [6.66672073e-04 4.25917506e-01 5.73415816e-01]\n",
            " [1.84035918e-03 9.86581802e-01 1.15778521e-02]\n",
            " [3.04963702e-04 2.77290225e-01 7.22404838e-01]\n",
            " [1.08074618e-03 8.65119517e-01 1.33799791e-01]\n",
            " [1.28250092e-03 9.83854830e-01 1.48626938e-02]\n",
            " [1.09342544e-03 9.83034253e-01 1.58722978e-02]\n",
            " [7.16586481e-04 8.94283891e-01 1.04999475e-01]\n",
            " [4.97864035e-04 6.01079345e-01 3.98422807e-01]\n",
            " [1.31452852e-03 8.17937255e-01 1.80748194e-01]\n",
            " [7.28235999e-03 9.89962757e-01 2.75483774e-03]\n",
            " [2.84804357e-03 9.65140700e-01 3.20112929e-02]\n",
            " [2.97213811e-03 9.83527124e-01 1.35006774e-02]\n",
            " [2.29411130e-03 9.84111547e-01 1.35943964e-02]\n",
            " [1.16392563e-04 9.54766944e-02 9.04406846e-01]\n",
            " [1.45271979e-03 6.18815124e-01 3.79732162e-01]\n",
            " [1.57650188e-03 9.43085432e-01 5.53380735e-02]\n",
            " [8.64726375e-04 9.50130165e-01 4.90050614e-02]\n",
            " [1.09602499e-03 8.00019443e-01 1.98884532e-01]\n",
            " [2.42004683e-03 9.70310986e-01 2.72690412e-02]\n",
            " [2.45596888e-03 8.78440797e-01 1.19103223e-01]\n",
            " [1.62329152e-03 7.44101346e-01 2.54275352e-01]\n",
            " [1.22877944e-03 8.91037941e-01 1.07733317e-01]\n",
            " [2.15457077e-03 9.68398631e-01 2.94466913e-02]\n",
            " [5.93636464e-03 9.82802331e-01 1.12612937e-02]\n",
            " [2.06383178e-03 8.86655271e-01 1.11280873e-01]\n",
            " [2.11553136e-03 9.76422310e-01 2.14621760e-02]\n",
            " [2.05027871e-03 9.51419413e-01 4.65303063e-02]\n",
            " [1.44552020e-03 9.75234568e-01 2.33198702e-02]\n",
            " [2.51376629e-02 9.70904708e-01 3.95762967e-03]\n",
            " [2.18319008e-03 9.54557776e-01 4.32589725e-02]\n",
            " [2.55189616e-06 4.24598344e-03 9.95751500e-01]\n",
            " [2.73928908e-05 1.79029983e-02 9.82069612e-01]\n",
            " [5.62386958e-06 1.25897732e-02 9.87404585e-01]\n",
            " [1.58734492e-05 1.96666904e-02 9.80317473e-01]\n",
            " [4.35927768e-06 6.18648669e-03 9.93809164e-01]\n",
            " [9.35372555e-07 4.34652809e-03 9.95652556e-01]\n",
            " [1.28095257e-04 3.59209739e-02 9.63950992e-01]\n",
            " [2.91534957e-06 9.95809399e-03 9.90038931e-01]\n",
            " [5.55744737e-06 9.78366472e-03 9.90210772e-01]\n",
            " [3.83797760e-06 9.41404700e-03 9.90582108e-01]\n",
            " [2.37388726e-04 2.46573150e-01 7.53189504e-01]\n",
            " [2.60031902e-05 2.80865338e-02 9.71887529e-01]\n",
            " [2.33539158e-05 3.48688439e-02 9.65107799e-01]\n",
            " [2.12785162e-05 1.22577352e-02 9.87721026e-01]\n",
            " [1.24650378e-05 8.76962673e-03 9.91217911e-01]\n",
            " [2.78652515e-05 2.76425518e-02 9.72329557e-01]\n",
            " [4.85884448e-05 6.53306097e-02 9.34620857e-01]\n",
            " [3.34913670e-06 1.72320884e-02 9.82764542e-01]\n",
            " [4.25588979e-07 2.65647750e-03 9.97343123e-01]\n",
            " [5.30608886e-05 4.11583371e-02 9.58788574e-01]\n",
            " [9.24318920e-06 1.56325679e-02 9.84358191e-01]\n",
            " [4.61325726e-05 2.34809145e-02 9.76472974e-01]\n",
            " [8.52964092e-07 4.39266628e-03 9.95606482e-01]\n",
            " [2.00182578e-04 1.68853238e-01 8.30946505e-01]\n",
            " [2.14304819e-05 3.40176374e-02 9.65960979e-01]\n",
            " [2.71567351e-05 7.56107643e-02 9.24362063e-01]\n",
            " [3.47289781e-04 2.62190521e-01 7.37462103e-01]\n",
            " [3.59783706e-04 2.72090465e-01 7.27549732e-01]\n",
            " [6.48584273e-06 7.87520967e-03 9.92118299e-01]\n",
            " [7.23754274e-05 1.89696580e-01 8.10231030e-01]\n",
            " [4.06055779e-06 1.28304120e-02 9.87165570e-01]\n",
            " [3.77729921e-05 1.99831843e-01 8.00130308e-01]\n",
            " [5.66555082e-06 7.00226659e-03 9.92992043e-01]\n",
            " [3.10400472e-04 3.19961667e-01 6.79727912e-01]\n",
            " [2.27651362e-05 2.65629012e-02 9.73414302e-01]\n",
            " [2.84346356e-06 9.81251802e-03 9.90184665e-01]\n",
            " [8.86797261e-06 9.26917884e-03 9.90721941e-01]\n",
            " [5.79261105e-05 7.35117346e-02 9.26430345e-01]\n",
            " [4.66957456e-04 3.14050674e-01 6.85482323e-01]\n",
            " [5.98554725e-05 9.09518600e-02 9.08988357e-01]\n",
            " [5.78636218e-06 7.72961788e-03 9.92264628e-01]\n",
            " [1.06142652e-04 1.29709169e-01 8.70184660e-01]\n",
            " [2.73928908e-05 1.79029983e-02 9.82069612e-01]\n",
            " [3.74902243e-06 6.47428725e-03 9.93521988e-01]\n",
            " [4.82648784e-06 6.64611859e-03 9.93349016e-01]\n",
            " [3.64315310e-05 4.10235673e-02 9.58940029e-01]\n",
            " [4.64327786e-05 3.95958088e-02 9.60357785e-01]\n",
            " [8.72109449e-05 9.43033919e-02 9.05609369e-01]\n",
            " [2.49133100e-05 2.32166611e-02 9.76758361e-01]\n",
            " [1.22296973e-04 9.04506743e-02 9.09427047e-01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2DafRdBnLVel"
      },
      "source": [
        "Biggest one is consoidered as classified. If you would like to turn of the scentific notation, the following line can."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKs4_Xt6LqKV"
      },
      "source": [
        "np.set_printoptions(suppress=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GmbES4DELzmb",
        "outputId": "4784f4a5-f410-41f2-98a5-1ff5c6a41a59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "print(y[0:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m7Bxn_ikMVzc",
        "outputId": "3b937554-6aea-4dc6-877c-675c93edd364",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "predict_classes = np.argmax(pred,axis=1)\n",
        "excepted_classes = np.argmax(y,axis=1)\n",
        "print(f\"Predictions: {predict_classes}\")\n",
        "print(f\"Excepted: {excepted_classes}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 1 2 1 2 1\n",
            " 1 1 1 1 1 1 1 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n",
            "Excepted: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
            " 2 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vOBGvsz_ND4X"
      },
      "source": [
        "we can also print out the actual name of the classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oZ-0PCbrNI9U",
        "outputId": "12ee9e18-6079-49e6-c89a-e5c78bed643c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "print(species[predict_classes[1:10])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
            "       'Iris-setosa', 'Iris-setosa', 'Iris-setosa', 'Iris-setosa',\n",
            "       'Iris-setosa'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8ioEn6XNr1l",
        "outputId": "24b47c7a-b49d-4f9d-87d4-58c2d9bade66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "correct = accuracy_score(excepted_classes, predict_classes)\n",
        "print(f\"Accuracy: {correct}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.9733333333333334\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWkfJhm6OfbY",
        "outputId": "1d29b8dc-ab58-489a-df97-86f14e07fd58",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Prediction by passing the values, also called adhoc prediction\n",
        "sample_flower = np.array([[5.0,3.0,4.0,2.0]], dtype=float)\n",
        "pred = model.predict(sample_flower)\n",
        "print(pred)\n",
        "pred = np.argmax(pred)\n",
        "print(f\"After argmax pred: {pred}\")\n",
        "print(f\"Predict that {sample_flower} is: {species[pred]}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00171089 0.3733349  0.6249542 ]]\n",
            "After argmax pred: 2\n",
            "Predict that [[5. 3. 4. 2.]] is: Iris-virginica\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-sjxiDexP2OR",
        "outputId": "23439119-3b9e-43bb-f2ab-e1080adc7907",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "# Predicts two sample flowers\n",
        "sample_flowers = np.array([[5.0,3.0,4.0,2.0],[5.2,3.5,1.5,0.8]], dtype=float)\n",
        "pred = model.predict(sample_flowers)\n",
        "print(pred)\n",
        "pred = np.argmax(pred,axis=1)\n",
        "print(f\"After argmax pred: {pred}\")\n",
        "print(f\"Predict that {sample_flowers} is: {species[pred]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.00171089 0.37333485 0.6249543 ]\n",
            " [0.99766266 0.00233717 0.00000008]]\n",
            "After argmax pred: [2 0]\n",
            "Predict that [[5.  3.  4.  2. ]\n",
            " [5.2 3.5 1.5 0.8]] is: Index(['Iris-virginica', 'Iris-setosa'], dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abfg3d7WSiNQ"
      },
      "source": [
        "**Saving and loading keras neural network**\n",
        "Complex neural network will take a long time to train/fit, it is helpful to save these neural networks so that they can be reloaded later. A reloaded neural network will not required training. Keras provides three format for neural network saving.\n",
        ">YAML\n",
        ">JSON\n",
        ">HDF5\n",
        "\n",
        "usually you will want to save in HDF5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dIbIjzjmUAfm",
        "outputId": "5b2df600-72fb-4934-bce5-f01b01531852",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "save_path = \"/content/sample_data\"\n",
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", na_values=['NA', '?'])\n",
        "\n",
        "cars = df['name']\n",
        "\n",
        "# Handle missing value\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "# Pandas as Numpy\n",
        "x = df[['cylinders','displacement','horsepower','weight','acceleration','year','origin']].values\n",
        "y = df['mpg'].values # regression\n",
        "\n",
        "# Build the network\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # output\n",
        "model.compile(loss = 'mean_squared_error', optimizer='adam')\n",
        "model.fit(x,y,verbose=2,epochs=100)\n",
        "\n",
        "# Predict\n",
        "pred = model.predict(x)\n",
        "\n",
        "# Measure RMSE error. RMSE is common for  regression\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
        "print(f\"Before save score (RMSE): {score}\")\n",
        "\n",
        "# save neural network model to JSON (no weights)\n",
        "model_json = model.to_json()\n",
        "with open(os.path.join(save_path,\"network.json\"), \"w\") as json_file:\n",
        "  json_file.write(model_json)\n",
        "\n",
        "\n",
        "# save neural network model to YAML (no weights)\n",
        "model_yaml = model.to_yaml()\n",
        "with open(os.path.join(save_path,\"network.yaml\"), \"w\") as yaml_file:\n",
        "  yaml_file.write(model_yaml)\n",
        "\n",
        "# save entire network to HDF5 (save everything suggested)\n",
        "model.save(os.path.join(save_path,\"network.h5\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "13/13 - 0s - loss: 432862.8438\n",
            "Epoch 2/100\n",
            "13/13 - 0s - loss: 89245.5859\n",
            "Epoch 3/100\n",
            "13/13 - 0s - loss: 11885.8906\n",
            "Epoch 4/100\n",
            "13/13 - 0s - loss: 524.5509\n",
            "Epoch 5/100\n",
            "13/13 - 0s - loss: 537.1743\n",
            "Epoch 6/100\n",
            "13/13 - 0s - loss: 391.7661\n",
            "Epoch 7/100\n",
            "13/13 - 0s - loss: 120.4809\n",
            "Epoch 8/100\n",
            "13/13 - 0s - loss: 97.7947\n",
            "Epoch 9/100\n",
            "13/13 - 0s - loss: 97.8201\n",
            "Epoch 10/100\n",
            "13/13 - 0s - loss: 93.6162\n",
            "Epoch 11/100\n",
            "13/13 - 0s - loss: 93.0865\n",
            "Epoch 12/100\n",
            "13/13 - 0s - loss: 92.4945\n",
            "Epoch 13/100\n",
            "13/13 - 0s - loss: 92.3102\n",
            "Epoch 14/100\n",
            "13/13 - 0s - loss: 92.1601\n",
            "Epoch 15/100\n",
            "13/13 - 0s - loss: 92.0598\n",
            "Epoch 16/100\n",
            "13/13 - 0s - loss: 91.9852\n",
            "Epoch 17/100\n",
            "13/13 - 0s - loss: 91.7845\n",
            "Epoch 18/100\n",
            "13/13 - 0s - loss: 91.6422\n",
            "Epoch 19/100\n",
            "13/13 - 0s - loss: 91.2977\n",
            "Epoch 20/100\n",
            "13/13 - 0s - loss: 91.2197\n",
            "Epoch 21/100\n",
            "13/13 - 0s - loss: 91.1344\n",
            "Epoch 22/100\n",
            "13/13 - 0s - loss: 90.8767\n",
            "Epoch 23/100\n",
            "13/13 - 0s - loss: 90.5898\n",
            "Epoch 24/100\n",
            "13/13 - 0s - loss: 90.5022\n",
            "Epoch 25/100\n",
            "13/13 - 0s - loss: 90.5886\n",
            "Epoch 26/100\n",
            "13/13 - 0s - loss: 90.1172\n",
            "Epoch 27/100\n",
            "13/13 - 0s - loss: 90.0467\n",
            "Epoch 28/100\n",
            "13/13 - 0s - loss: 89.6197\n",
            "Epoch 29/100\n",
            "13/13 - 0s - loss: 90.0552\n",
            "Epoch 30/100\n",
            "13/13 - 0s - loss: 89.2755\n",
            "Epoch 31/100\n",
            "13/13 - 0s - loss: 89.0602\n",
            "Epoch 32/100\n",
            "13/13 - 0s - loss: 88.7330\n",
            "Epoch 33/100\n",
            "13/13 - 0s - loss: 88.8064\n",
            "Epoch 34/100\n",
            "13/13 - 0s - loss: 88.8787\n",
            "Epoch 35/100\n",
            "13/13 - 0s - loss: 88.2792\n",
            "Epoch 36/100\n",
            "13/13 - 0s - loss: 88.3508\n",
            "Epoch 37/100\n",
            "13/13 - 0s - loss: 87.6108\n",
            "Epoch 38/100\n",
            "13/13 - 0s - loss: 87.5409\n",
            "Epoch 39/100\n",
            "13/13 - 0s - loss: 87.5175\n",
            "Epoch 40/100\n",
            "13/13 - 0s - loss: 88.0021\n",
            "Epoch 41/100\n",
            "13/13 - 0s - loss: 86.9367\n",
            "Epoch 42/100\n",
            "13/13 - 0s - loss: 86.6091\n",
            "Epoch 43/100\n",
            "13/13 - 0s - loss: 86.7820\n",
            "Epoch 44/100\n",
            "13/13 - 0s - loss: 86.9398\n",
            "Epoch 45/100\n",
            "13/13 - 0s - loss: 86.4030\n",
            "Epoch 46/100\n",
            "13/13 - 0s - loss: 85.4484\n",
            "Epoch 47/100\n",
            "13/13 - 0s - loss: 85.8129\n",
            "Epoch 48/100\n",
            "13/13 - 0s - loss: 85.7006\n",
            "Epoch 49/100\n",
            "13/13 - 0s - loss: 85.0837\n",
            "Epoch 50/100\n",
            "13/13 - 0s - loss: 84.6759\n",
            "Epoch 51/100\n",
            "13/13 - 0s - loss: 84.4435\n",
            "Epoch 52/100\n",
            "13/13 - 0s - loss: 84.1470\n",
            "Epoch 53/100\n",
            "13/13 - 0s - loss: 83.8597\n",
            "Epoch 54/100\n",
            "13/13 - 0s - loss: 83.5691\n",
            "Epoch 55/100\n",
            "13/13 - 0s - loss: 84.0801\n",
            "Epoch 56/100\n",
            "13/13 - 0s - loss: 83.4632\n",
            "Epoch 57/100\n",
            "13/13 - 0s - loss: 82.7522\n",
            "Epoch 58/100\n",
            "13/13 - 0s - loss: 82.5186\n",
            "Epoch 59/100\n",
            "13/13 - 0s - loss: 82.3402\n",
            "Epoch 60/100\n",
            "13/13 - 0s - loss: 82.1086\n",
            "Epoch 61/100\n",
            "13/13 - 0s - loss: 81.9210\n",
            "Epoch 62/100\n",
            "13/13 - 0s - loss: 81.6808\n",
            "Epoch 63/100\n",
            "13/13 - 0s - loss: 81.4006\n",
            "Epoch 64/100\n",
            "13/13 - 0s - loss: 81.3055\n",
            "Epoch 65/100\n",
            "13/13 - 0s - loss: 80.8776\n",
            "Epoch 66/100\n",
            "13/13 - 0s - loss: 80.7212\n",
            "Epoch 67/100\n",
            "13/13 - 0s - loss: 80.1466\n",
            "Epoch 68/100\n",
            "13/13 - 0s - loss: 80.4407\n",
            "Epoch 69/100\n",
            "13/13 - 0s - loss: 79.7512\n",
            "Epoch 70/100\n",
            "13/13 - 0s - loss: 79.6036\n",
            "Epoch 71/100\n",
            "13/13 - 0s - loss: 79.7598\n",
            "Epoch 72/100\n",
            "13/13 - 0s - loss: 79.1398\n",
            "Epoch 73/100\n",
            "13/13 - 0s - loss: 78.6603\n",
            "Epoch 74/100\n",
            "13/13 - 0s - loss: 78.5054\n",
            "Epoch 75/100\n",
            "13/13 - 0s - loss: 78.1021\n",
            "Epoch 76/100\n",
            "13/13 - 0s - loss: 78.3784\n",
            "Epoch 77/100\n",
            "13/13 - 0s - loss: 77.7815\n",
            "Epoch 78/100\n",
            "13/13 - 0s - loss: 77.4466\n",
            "Epoch 79/100\n",
            "13/13 - 0s - loss: 77.2167\n",
            "Epoch 80/100\n",
            "13/13 - 0s - loss: 76.9285\n",
            "Epoch 81/100\n",
            "13/13 - 0s - loss: 76.5468\n",
            "Epoch 82/100\n",
            "13/13 - 0s - loss: 76.4180\n",
            "Epoch 83/100\n",
            "13/13 - 0s - loss: 76.1085\n",
            "Epoch 84/100\n",
            "13/13 - 0s - loss: 76.0412\n",
            "Epoch 85/100\n",
            "13/13 - 0s - loss: 75.7132\n",
            "Epoch 86/100\n",
            "13/13 - 0s - loss: 75.5655\n",
            "Epoch 87/100\n",
            "13/13 - 0s - loss: 75.4249\n",
            "Epoch 88/100\n",
            "13/13 - 0s - loss: 75.8637\n",
            "Epoch 89/100\n",
            "13/13 - 0s - loss: 74.3518\n",
            "Epoch 90/100\n",
            "13/13 - 0s - loss: 75.0509\n",
            "Epoch 91/100\n",
            "13/13 - 0s - loss: 74.9360\n",
            "Epoch 92/100\n",
            "13/13 - 0s - loss: 76.0762\n",
            "Epoch 93/100\n",
            "13/13 - 0s - loss: 74.0424\n",
            "Epoch 94/100\n",
            "13/13 - 0s - loss: 73.6457\n",
            "Epoch 95/100\n",
            "13/13 - 0s - loss: 73.5804\n",
            "Epoch 96/100\n",
            "13/13 - 0s - loss: 73.6995\n",
            "Epoch 97/100\n",
            "13/13 - 0s - loss: 73.2005\n",
            "Epoch 98/100\n",
            "13/13 - 0s - loss: 72.6343\n",
            "Epoch 99/100\n",
            "13/13 - 0s - loss: 73.2327\n",
            "Epoch 100/100\n",
            "13/13 - 0s - loss: 72.3534\n",
            "Before save score (RMSE): 8.484526573737398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G81x1HYWoRHW"
      },
      "source": [
        "Now we reload the network and perform another prediction. The RMSE sould match the previous one exactly is the neural network was really saved and reloaded.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2prLIimopBS",
        "outputId": "2a591029-5b1a-452d-b165-0e1ebf76819c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model2 = load_model(os.path.join(save_path,\"network.h5\"))\n",
        "pred = model2.predict(x)\n",
        "# measure RMSE error. RMSE is common for regression.\n",
        "score = np.sqrt(metrics.mean_squared_error(pred,y))\n",
        "print(f\"After load score (RMSE): {score}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "After load score (RMSE): 8.484526573737398\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xf1bjXhbBU4D"
      },
      "source": [
        "**Early stopping in Keras to prevent overfitting**\n",
        "\n",
        "For the divide the dataset into training and validation set, generally we keep 80% for training and 20% for validation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tAuuRq_KDfPA",
        "outputId": "13f30ecc-790a-4681-8adc-a63eb5cd95e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import io\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/iris.csv\", na_values=['NA', '?'])\n",
        "\n",
        "# Convert to numpy - classification\n",
        "x=df[['sepal_l','sepal_w','petal_l','petal_w']].values\n",
        "dummies = pd.get_dummies(df['species']) # classification\n",
        "species = dummies.columns\n",
        "y = dummies.values\n",
        "\n",
        "# Split into validation and training set\n",
        "x_train,x_test, y_train,y_test= train_test_split(x,y, test_size = 0.25, random_state = 42)\n",
        "\n",
        "# Build neural network\n",
        "model =  Sequential()\n",
        "model.add(Dense(50, input_dim=x.shape[1],activation='relu')) # Hidden 1\n",
        "model.add(Dense(25, activation='relu')) # Hidden 2\n",
        "model.add(Dense(y.shape[1],activation='softmax')) # Output\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss',min_delta=1e-3, patience=5,verbose=1, mode='auto',restore_best_weights=True)\n",
        "model.fit(x_train,y_train, validation_data = (x_test,y_test), callbacks=[monitor], verbose=2, epochs=1000)\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "4/4 - 0s - loss: 1.4397 - val_loss: 1.2367\n",
            "Epoch 2/1000\n",
            "4/4 - 0s - loss: 1.2896 - val_loss: 1.1330\n",
            "Epoch 3/1000\n",
            "4/4 - 0s - loss: 1.1743 - val_loss: 1.0474\n",
            "Epoch 4/1000\n",
            "4/4 - 0s - loss: 1.0904 - val_loss: 0.9952\n",
            "Epoch 5/1000\n",
            "4/4 - 0s - loss: 1.0254 - val_loss: 0.9468\n",
            "Epoch 6/1000\n",
            "4/4 - 0s - loss: 0.9672 - val_loss: 0.9050\n",
            "Epoch 7/1000\n",
            "4/4 - 0s - loss: 0.9197 - val_loss: 0.8711\n",
            "Epoch 8/1000\n",
            "4/4 - 0s - loss: 0.8854 - val_loss: 0.8378\n",
            "Epoch 9/1000\n",
            "4/4 - 0s - loss: 0.8439 - val_loss: 0.8011\n",
            "Epoch 10/1000\n",
            "4/4 - 0s - loss: 0.8078 - val_loss: 0.7704\n",
            "Epoch 11/1000\n",
            "4/4 - 0s - loss: 0.7765 - val_loss: 0.7408\n",
            "Epoch 12/1000\n",
            "4/4 - 0s - loss: 0.7439 - val_loss: 0.7121\n",
            "Epoch 13/1000\n",
            "4/4 - 0s - loss: 0.7156 - val_loss: 0.6803\n",
            "Epoch 14/1000\n",
            "4/4 - 0s - loss: 0.6826 - val_loss: 0.6438\n",
            "Epoch 15/1000\n",
            "4/4 - 0s - loss: 0.6496 - val_loss: 0.6092\n",
            "Epoch 16/1000\n",
            "4/4 - 0s - loss: 0.6202 - val_loss: 0.5798\n",
            "Epoch 17/1000\n",
            "4/4 - 0s - loss: 0.5919 - val_loss: 0.5515\n",
            "Epoch 18/1000\n",
            "4/4 - 0s - loss: 0.5643 - val_loss: 0.5254\n",
            "Epoch 19/1000\n",
            "4/4 - 0s - loss: 0.5405 - val_loss: 0.4982\n",
            "Epoch 20/1000\n",
            "4/4 - 0s - loss: 0.5133 - val_loss: 0.4703\n",
            "Epoch 21/1000\n",
            "4/4 - 0s - loss: 0.4880 - val_loss: 0.4444\n",
            "Epoch 22/1000\n",
            "4/4 - 0s - loss: 0.4645 - val_loss: 0.4209\n",
            "Epoch 23/1000\n",
            "4/4 - 0s - loss: 0.4435 - val_loss: 0.3982\n",
            "Epoch 24/1000\n",
            "4/4 - 0s - loss: 0.4211 - val_loss: 0.3794\n",
            "Epoch 25/1000\n",
            "4/4 - 0s - loss: 0.4044 - val_loss: 0.3628\n",
            "Epoch 26/1000\n",
            "4/4 - 0s - loss: 0.3863 - val_loss: 0.3434\n",
            "Epoch 27/1000\n",
            "4/4 - 0s - loss: 0.3686 - val_loss: 0.3250\n",
            "Epoch 28/1000\n",
            "4/4 - 0s - loss: 0.3534 - val_loss: 0.3098\n",
            "Epoch 29/1000\n",
            "4/4 - 0s - loss: 0.3385 - val_loss: 0.2972\n",
            "Epoch 30/1000\n",
            "4/4 - 0s - loss: 0.3285 - val_loss: 0.2860\n",
            "Epoch 31/1000\n",
            "4/4 - 0s - loss: 0.3125 - val_loss: 0.2729\n",
            "Epoch 32/1000\n",
            "4/4 - 0s - loss: 0.3027 - val_loss: 0.2616\n",
            "Epoch 33/1000\n",
            "4/4 - 0s - loss: 0.2921 - val_loss: 0.2526\n",
            "Epoch 34/1000\n",
            "4/4 - 0s - loss: 0.2819 - val_loss: 0.2429\n",
            "Epoch 35/1000\n",
            "4/4 - 0s - loss: 0.2717 - val_loss: 0.2338\n",
            "Epoch 36/1000\n",
            "4/4 - 0s - loss: 0.2632 - val_loss: 0.2260\n",
            "Epoch 37/1000\n",
            "4/4 - 0s - loss: 0.2545 - val_loss: 0.2203\n",
            "Epoch 38/1000\n",
            "4/4 - 0s - loss: 0.2472 - val_loss: 0.2141\n",
            "Epoch 39/1000\n",
            "4/4 - 0s - loss: 0.2391 - val_loss: 0.2055\n",
            "Epoch 40/1000\n",
            "4/4 - 0s - loss: 0.2321 - val_loss: 0.1982\n",
            "Epoch 41/1000\n",
            "4/4 - 0s - loss: 0.2263 - val_loss: 0.1926\n",
            "Epoch 42/1000\n",
            "4/4 - 0s - loss: 0.2199 - val_loss: 0.1889\n",
            "Epoch 43/1000\n",
            "4/4 - 0s - loss: 0.2136 - val_loss: 0.1821\n",
            "Epoch 44/1000\n",
            "4/4 - 0s - loss: 0.2079 - val_loss: 0.1751\n",
            "Epoch 45/1000\n",
            "4/4 - 0s - loss: 0.2039 - val_loss: 0.1700\n",
            "Epoch 46/1000\n",
            "4/4 - 0s - loss: 0.1962 - val_loss: 0.1690\n",
            "Epoch 47/1000\n",
            "4/4 - 0s - loss: 0.1969 - val_loss: 0.1685\n",
            "Epoch 48/1000\n",
            "4/4 - 0s - loss: 0.1854 - val_loss: 0.1562\n",
            "Epoch 49/1000\n",
            "4/4 - 0s - loss: 0.1868 - val_loss: 0.1528\n",
            "Epoch 50/1000\n",
            "4/4 - 0s - loss: 0.1798 - val_loss: 0.1519\n",
            "Epoch 51/1000\n",
            "4/4 - 0s - loss: 0.1750 - val_loss: 0.1544\n",
            "Epoch 52/1000\n",
            "4/4 - 0s - loss: 0.1706 - val_loss: 0.1437\n",
            "Epoch 53/1000\n",
            "4/4 - 0s - loss: 0.1660 - val_loss: 0.1382\n",
            "Epoch 54/1000\n",
            "4/4 - 0s - loss: 0.1628 - val_loss: 0.1375\n",
            "Epoch 55/1000\n",
            "4/4 - 0s - loss: 0.1583 - val_loss: 0.1402\n",
            "Epoch 56/1000\n",
            "4/4 - 0s - loss: 0.1557 - val_loss: 0.1349\n",
            "Epoch 57/1000\n",
            "4/4 - 0s - loss: 0.1511 - val_loss: 0.1270\n",
            "Epoch 58/1000\n",
            "4/4 - 0s - loss: 0.1486 - val_loss: 0.1249\n",
            "Epoch 59/1000\n",
            "4/4 - 0s - loss: 0.1465 - val_loss: 0.1265\n",
            "Epoch 60/1000\n",
            "4/4 - 0s - loss: 0.1453 - val_loss: 0.1199\n",
            "Epoch 61/1000\n",
            "4/4 - 0s - loss: 0.1410 - val_loss: 0.1224\n",
            "Epoch 62/1000\n",
            "4/4 - 0s - loss: 0.1375 - val_loss: 0.1206\n",
            "Epoch 63/1000\n",
            "4/4 - 0s - loss: 0.1356 - val_loss: 0.1147\n",
            "Epoch 64/1000\n",
            "4/4 - 0s - loss: 0.1331 - val_loss: 0.1112\n",
            "Epoch 65/1000\n",
            "4/4 - 0s - loss: 0.1307 - val_loss: 0.1132\n",
            "Epoch 66/1000\n",
            "4/4 - 0s - loss: 0.1294 - val_loss: 0.1096\n",
            "Epoch 67/1000\n",
            "4/4 - 0s - loss: 0.1282 - val_loss: 0.1053\n",
            "Epoch 68/1000\n",
            "4/4 - 0s - loss: 0.1262 - val_loss: 0.1092\n",
            "Epoch 69/1000\n",
            "4/4 - 0s - loss: 0.1235 - val_loss: 0.1048\n",
            "Epoch 70/1000\n",
            "4/4 - 0s - loss: 0.1212 - val_loss: 0.1042\n",
            "Epoch 71/1000\n",
            "4/4 - 0s - loss: 0.1193 - val_loss: 0.1019\n",
            "Epoch 72/1000\n",
            "4/4 - 0s - loss: 0.1175 - val_loss: 0.0987\n",
            "Epoch 73/1000\n",
            "4/4 - 0s - loss: 0.1168 - val_loss: 0.0977\n",
            "Epoch 74/1000\n",
            "4/4 - 0s - loss: 0.1137 - val_loss: 0.1029\n",
            "Epoch 75/1000\n",
            "4/4 - 0s - loss: 0.1173 - val_loss: 0.1054\n",
            "Epoch 76/1000\n",
            "4/4 - 0s - loss: 0.1122 - val_loss: 0.0914\n",
            "Epoch 77/1000\n",
            "4/4 - 0s - loss: 0.1145 - val_loss: 0.0898\n",
            "Epoch 78/1000\n",
            "4/4 - 0s - loss: 0.1100 - val_loss: 0.0924\n",
            "Epoch 79/1000\n",
            "4/4 - 0s - loss: 0.1074 - val_loss: 0.0964\n",
            "Epoch 80/1000\n",
            "4/4 - 0s - loss: 0.1076 - val_loss: 0.0950\n",
            "Epoch 81/1000\n",
            "4/4 - 0s - loss: 0.1062 - val_loss: 0.0901\n",
            "Epoch 82/1000\n",
            "4/4 - 0s - loss: 0.1042 - val_loss: 0.0884\n",
            "Epoch 83/1000\n",
            "4/4 - 0s - loss: 0.1043 - val_loss: 0.0849\n",
            "Epoch 84/1000\n",
            "4/4 - 0s - loss: 0.1029 - val_loss: 0.0869\n",
            "Epoch 85/1000\n",
            "4/4 - 0s - loss: 0.1064 - val_loss: 0.0919\n",
            "Epoch 86/1000\n",
            "4/4 - 0s - loss: 0.1042 - val_loss: 0.0806\n",
            "Epoch 87/1000\n",
            "4/4 - 0s - loss: 0.1017 - val_loss: 0.0887\n",
            "Epoch 88/1000\n",
            "4/4 - 0s - loss: 0.0990 - val_loss: 0.0901\n",
            "Epoch 89/1000\n",
            "4/4 - 0s - loss: 0.1003 - val_loss: 0.0836\n",
            "Epoch 90/1000\n",
            "4/4 - 0s - loss: 0.0989 - val_loss: 0.0822\n",
            "Epoch 91/1000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "4/4 - 0s - loss: 0.1030 - val_loss: 0.0929\n",
            "Epoch 00091: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f1aec6630>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go9KP8e3Io18"
      },
      "source": [
        "Lets find the accuracy score"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjgpTkHRIrTJ",
        "outputId": "b79ad6a3-c8b6-4eb2-9f86-64992a4dc0c4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 714
        }
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "pred = model.predict(x_test)\n",
        "display(pred)\n",
        "predict_classes = np.argmax(pred,axis=1)\n",
        "display(predict_classes)\n",
        "expected_classes = np.argmax(y_test,axis=1)\n",
        "correct = accuracy_score(expected_classes, predict_classes)\n",
        "print(f\"Accuracy: {correct}\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([[5.7823895e-03, 8.9392495e-01, 1.0029259e-01],\n",
              "       [9.9203134e-01, 7.9616224e-03, 7.1088166e-06],\n",
              "       [2.7892147e-06, 1.0698487e-03, 9.9892730e-01],\n",
              "       [8.0848774e-03, 8.4978193e-01, 1.4213316e-01],\n",
              "       [5.2781091e-03, 9.4439751e-01, 5.0324351e-02],\n",
              "       [9.8946172e-01, 1.0523994e-02, 1.4247887e-05],\n",
              "       [4.1217320e-02, 9.4994187e-01, 8.8408170e-03],\n",
              "       [1.4887433e-03, 2.1493025e-01, 7.8358102e-01],\n",
              "       [4.5105633e-03, 5.6193733e-01, 4.3355206e-01],\n",
              "       [1.8126762e-02, 9.6855372e-01, 1.3319487e-02],\n",
              "       [1.9680467e-03, 2.9847282e-01, 6.9955915e-01],\n",
              "       [9.8472637e-01, 1.5236213e-02, 3.7410227e-05],\n",
              "       [9.9546510e-01, 4.5299423e-03, 4.9533828e-06],\n",
              "       [9.8359030e-01, 1.6373655e-02, 3.6041853e-05],\n",
              "       [9.9190372e-01, 8.0842981e-03, 1.1977205e-05],\n",
              "       [7.6545775e-03, 9.1860539e-01, 7.3740020e-02],\n",
              "       [2.9059762e-05, 5.0493600e-03, 9.9492162e-01],\n",
              "       [1.4405219e-02, 9.6479422e-01, 2.0800607e-02],\n",
              "       [7.2719748e-03, 8.0784893e-01, 1.8487914e-01],\n",
              "       [3.5550111e-05, 5.2218987e-03, 9.9474251e-01],\n",
              "       [9.7382486e-01, 2.6104694e-02, 7.0518043e-05],\n",
              "       [2.3186523e-03, 3.0320805e-01, 6.9447327e-01],\n",
              "       [9.7993970e-01, 2.0020053e-02, 4.0306335e-05],\n",
              "       [4.6471469e-05, 7.5405752e-03, 9.9241298e-01],\n",
              "       [4.4968340e-04, 2.8525558e-01, 7.1429473e-01],\n",
              "       [4.6674508e-04, 6.5725982e-02, 9.3380725e-01],\n",
              "       [5.2731211e-05, 1.2955305e-02, 9.8699200e-01],\n",
              "       [3.3592572e-05, 7.2533707e-03, 9.9271303e-01],\n",
              "       [9.8092127e-01, 1.9029155e-02, 4.9614802e-05],\n",
              "       [9.7317082e-01, 2.6760666e-02, 6.8463800e-05],\n",
              "       [9.9575365e-01, 4.2369319e-03, 9.3922163e-06],\n",
              "       [9.9725097e-01, 2.7470917e-03, 1.9077888e-06],\n",
              "       [1.2035900e-02, 9.8031074e-01, 7.6533658e-03],\n",
              "       [9.8073822e-01, 1.9218070e-02, 4.3754870e-05],\n",
              "       [9.8490727e-01, 1.5043040e-02, 4.9707931e-05],\n",
              "       [4.7894006e-04, 6.6330723e-02, 9.3319029e-01],\n",
              "       [8.9270761e-03, 9.7155905e-01, 1.9513864e-02],\n",
              "       [9.9114579e-01, 8.8412166e-03, 1.3005479e-05]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "array([1, 0, 2, 1, 1, 0, 1, 2, 1, 1, 2, 0, 0, 0, 0, 1, 2, 1, 1, 2, 0, 2,\n",
              "       0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 1, 0, 0, 2, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bh4FhykKTsq"
      },
      "source": [
        "**Early Stopping with regression**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JzZxcur5KX_p",
        "outputId": "9eee8d18-0a21-4d5d-dd81-d3edc29d1623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "\n",
        "df = pd.read_csv(\n",
        "    \"https://data.heatonresearch.com/data/t81-558/auto-mpg.csv\", \n",
        "    na_values=['NA', '?'])\n",
        "\n",
        "cars = df['name']\n",
        "\n",
        "# Handle missing value\n",
        "df['horsepower'] = df['horsepower'].fillna(df['horsepower'].median())\n",
        "\n",
        "# Pandas to Numpy\n",
        "x = df[['cylinders', 'displacement', 'horsepower', 'weight',\n",
        "       'acceleration', 'year', 'origin']].values\n",
        "y = df['mpg'].values # regression\n",
        "\n",
        "# Split into validation and training sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(    \n",
        "    x, y, test_size=0.25, random_state=42)\n",
        "\n",
        "# Build the neural network\n",
        "model = Sequential()\n",
        "model.add(Dense(25, input_dim=x.shape[1], activation='relu')) # Hidden 1\n",
        "model.add(Dense(10, activation='relu')) # Hidden 2\n",
        "model.add(Dense(1)) # Output\n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "\n",
        "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, \n",
        "        patience=5, verbose=1, mode='auto',\n",
        "        restore_best_weights=True)\n",
        "model.fit(x_train,y_train,validation_data=(x_test,y_test),\n",
        "        callbacks=[monitor], verbose=2,epochs=1000)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/1000\n",
            "10/10 - 0s - loss: 231613.0156 - val_loss: 127209.4531\n",
            "Epoch 2/1000\n",
            "10/10 - 0s - loss: 86646.6719 - val_loss: 36668.8008\n",
            "Epoch 3/1000\n",
            "10/10 - 0s - loss: 21600.1602 - val_loss: 5971.6592\n",
            "Epoch 4/1000\n",
            "10/10 - 0s - loss: 3218.9382 - val_loss: 1800.3075\n",
            "Epoch 5/1000\n",
            "10/10 - 0s - loss: 1775.3948 - val_loss: 2575.7092\n",
            "Epoch 6/1000\n",
            "10/10 - 0s - loss: 2128.1880 - val_loss: 2445.1497\n",
            "Epoch 7/1000\n",
            "10/10 - 0s - loss: 1794.0801 - val_loss: 1971.4465\n",
            "Epoch 8/1000\n",
            "10/10 - 0s - loss: 1476.7157 - val_loss: 1686.4865\n",
            "Epoch 9/1000\n",
            "10/10 - 0s - loss: 1383.1962 - val_loss: 1613.4393\n",
            "Epoch 10/1000\n",
            "10/10 - 0s - loss: 1364.3843 - val_loss: 1585.1436\n",
            "Epoch 11/1000\n",
            "10/10 - 0s - loss: 1345.7542 - val_loss: 1560.3682\n",
            "Epoch 12/1000\n",
            "10/10 - 0s - loss: 1318.4625 - val_loss: 1539.2306\n",
            "Epoch 13/1000\n",
            "10/10 - 0s - loss: 1298.6498 - val_loss: 1515.7964\n",
            "Epoch 14/1000\n",
            "10/10 - 0s - loss: 1273.4081 - val_loss: 1472.8169\n",
            "Epoch 15/1000\n",
            "10/10 - 0s - loss: 1237.9231 - val_loss: 1416.1442\n",
            "Epoch 16/1000\n",
            "10/10 - 0s - loss: 1195.6130 - val_loss: 1348.3080\n",
            "Epoch 17/1000\n",
            "10/10 - 0s - loss: 1135.1702 - val_loss: 1271.3225\n",
            "Epoch 18/1000\n",
            "10/10 - 0s - loss: 1064.6061 - val_loss: 1179.3582\n",
            "Epoch 19/1000\n",
            "10/10 - 0s - loss: 985.2460 - val_loss: 1079.9469\n",
            "Epoch 20/1000\n",
            "10/10 - 0s - loss: 899.7665 - val_loss: 991.9072\n",
            "Epoch 21/1000\n",
            "10/10 - 0s - loss: 817.8975 - val_loss: 901.1805\n",
            "Epoch 22/1000\n",
            "10/10 - 0s - loss: 741.8588 - val_loss: 815.2293\n",
            "Epoch 23/1000\n",
            "10/10 - 0s - loss: 670.3036 - val_loss: 734.0762\n",
            "Epoch 24/1000\n",
            "10/10 - 0s - loss: 604.4223 - val_loss: 598.9397\n",
            "Epoch 25/1000\n",
            "10/10 - 0s - loss: 465.8025 - val_loss: 433.2100\n",
            "Epoch 26/1000\n",
            "10/10 - 0s - loss: 392.5797 - val_loss: 378.9442\n",
            "Epoch 27/1000\n",
            "10/10 - 0s - loss: 333.2332 - val_loss: 342.5674\n",
            "Epoch 28/1000\n",
            "10/10 - 0s - loss: 285.3707 - val_loss: 288.9239\n",
            "Epoch 29/1000\n",
            "10/10 - 0s - loss: 260.7714 - val_loss: 253.5640\n",
            "Epoch 30/1000\n",
            "10/10 - 0s - loss: 226.7143 - val_loss: 217.5412\n",
            "Epoch 31/1000\n",
            "10/10 - 0s - loss: 198.4670 - val_loss: 189.1963\n",
            "Epoch 32/1000\n",
            "10/10 - 0s - loss: 175.7632 - val_loss: 165.6498\n",
            "Epoch 33/1000\n",
            "10/10 - 0s - loss: 156.2092 - val_loss: 143.4222\n",
            "Epoch 34/1000\n",
            "10/10 - 0s - loss: 140.8477 - val_loss: 125.8582\n",
            "Epoch 35/1000\n",
            "10/10 - 0s - loss: 125.9084 - val_loss: 112.8810\n",
            "Epoch 36/1000\n",
            "10/10 - 0s - loss: 117.4279 - val_loss: 101.7473\n",
            "Epoch 37/1000\n",
            "10/10 - 0s - loss: 109.1980 - val_loss: 92.9297\n",
            "Epoch 38/1000\n",
            "10/10 - 0s - loss: 100.1723 - val_loss: 83.9172\n",
            "Epoch 39/1000\n",
            "10/10 - 0s - loss: 94.7694 - val_loss: 80.5601\n",
            "Epoch 40/1000\n",
            "10/10 - 0s - loss: 90.0591 - val_loss: 73.7837\n",
            "Epoch 41/1000\n",
            "10/10 - 0s - loss: 86.5538 - val_loss: 71.8485\n",
            "Epoch 42/1000\n",
            "10/10 - 0s - loss: 83.6709 - val_loss: 66.0749\n",
            "Epoch 43/1000\n",
            "10/10 - 0s - loss: 80.6731 - val_loss: 69.9646\n",
            "Epoch 44/1000\n",
            "10/10 - 0s - loss: 78.7215 - val_loss: 60.9355\n",
            "Epoch 45/1000\n",
            "10/10 - 0s - loss: 76.1492 - val_loss: 60.6418\n",
            "Epoch 46/1000\n",
            "10/10 - 0s - loss: 75.5826 - val_loss: 64.3072\n",
            "Epoch 47/1000\n",
            "10/10 - 0s - loss: 72.4635 - val_loss: 56.8647\n",
            "Epoch 48/1000\n",
            "10/10 - 0s - loss: 70.2506 - val_loss: 64.7369\n",
            "Epoch 49/1000\n",
            "10/10 - 0s - loss: 70.1240 - val_loss: 53.4428\n",
            "Epoch 50/1000\n",
            "10/10 - 0s - loss: 68.6150 - val_loss: 52.2260\n",
            "Epoch 51/1000\n",
            "10/10 - 0s - loss: 70.2875 - val_loss: 59.0996\n",
            "Epoch 52/1000\n",
            "10/10 - 0s - loss: 66.0320 - val_loss: 52.1344\n",
            "Epoch 53/1000\n",
            "10/10 - 0s - loss: 63.8723 - val_loss: 49.6425\n",
            "Epoch 54/1000\n",
            "10/10 - 0s - loss: 64.1230 - val_loss: 57.2384\n",
            "Epoch 55/1000\n",
            "10/10 - 0s - loss: 61.9439 - val_loss: 47.2305\n",
            "Epoch 56/1000\n",
            "10/10 - 0s - loss: 61.2104 - val_loss: 46.4711\n",
            "Epoch 57/1000\n",
            "10/10 - 0s - loss: 58.8123 - val_loss: 49.1641\n",
            "Epoch 58/1000\n",
            "10/10 - 0s - loss: 57.9645 - val_loss: 45.3331\n",
            "Epoch 59/1000\n",
            "10/10 - 0s - loss: 57.3689 - val_loss: 43.9871\n",
            "Epoch 60/1000\n",
            "10/10 - 0s - loss: 55.7549 - val_loss: 44.1782\n",
            "Epoch 61/1000\n",
            "10/10 - 0s - loss: 55.3848 - val_loss: 45.4528\n",
            "Epoch 62/1000\n",
            "10/10 - 0s - loss: 53.2914 - val_loss: 41.7707\n",
            "Epoch 63/1000\n",
            "10/10 - 0s - loss: 54.1440 - val_loss: 47.8533\n",
            "Epoch 64/1000\n",
            "10/10 - 0s - loss: 53.5662 - val_loss: 42.9493\n",
            "Epoch 65/1000\n",
            "10/10 - 0s - loss: 51.6191 - val_loss: 40.0521\n",
            "Epoch 66/1000\n",
            "10/10 - 0s - loss: 50.4627 - val_loss: 39.1942\n",
            "Epoch 67/1000\n",
            "10/10 - 0s - loss: 50.5659 - val_loss: 38.5507\n",
            "Epoch 68/1000\n",
            "10/10 - 0s - loss: 49.2960 - val_loss: 40.7755\n",
            "Epoch 69/1000\n",
            "10/10 - 0s - loss: 48.4673 - val_loss: 38.3597\n",
            "Epoch 70/1000\n",
            "10/10 - 0s - loss: 48.0735 - val_loss: 36.7882\n",
            "Epoch 71/1000\n",
            "10/10 - 0s - loss: 46.0443 - val_loss: 40.6508\n",
            "Epoch 72/1000\n",
            "10/10 - 0s - loss: 48.5721 - val_loss: 36.7346\n",
            "Epoch 73/1000\n",
            "10/10 - 0s - loss: 45.7439 - val_loss: 35.2849\n",
            "Epoch 74/1000\n",
            "10/10 - 0s - loss: 43.4980 - val_loss: 37.9109\n",
            "Epoch 75/1000\n",
            "10/10 - 0s - loss: 44.0664 - val_loss: 34.7062\n",
            "Epoch 76/1000\n",
            "10/10 - 0s - loss: 42.9456 - val_loss: 33.7127\n",
            "Epoch 77/1000\n",
            "10/10 - 0s - loss: 42.3791 - val_loss: 35.8196\n",
            "Epoch 78/1000\n",
            "10/10 - 0s - loss: 41.6815 - val_loss: 34.6709\n",
            "Epoch 79/1000\n",
            "10/10 - 0s - loss: 41.3586 - val_loss: 32.4300\n",
            "Epoch 80/1000\n",
            "10/10 - 0s - loss: 40.4249 - val_loss: 40.5979\n",
            "Epoch 81/1000\n",
            "10/10 - 0s - loss: 42.5879 - val_loss: 33.1965\n",
            "Epoch 82/1000\n",
            "10/10 - 0s - loss: 38.3997 - val_loss: 37.4354\n",
            "Epoch 83/1000\n",
            "10/10 - 0s - loss: 39.4763 - val_loss: 31.7056\n",
            "Epoch 84/1000\n",
            "10/10 - 0s - loss: 38.9287 - val_loss: 31.8579\n",
            "Epoch 85/1000\n",
            "10/10 - 0s - loss: 36.9753 - val_loss: 30.1306\n",
            "Epoch 86/1000\n",
            "10/10 - 0s - loss: 36.7422 - val_loss: 32.4271\n",
            "Epoch 87/1000\n",
            "10/10 - 0s - loss: 37.7111 - val_loss: 29.5312\n",
            "Epoch 88/1000\n",
            "10/10 - 0s - loss: 35.5285 - val_loss: 29.1497\n",
            "Epoch 89/1000\n",
            "10/10 - 0s - loss: 35.3068 - val_loss: 28.6205\n",
            "Epoch 90/1000\n",
            "10/10 - 0s - loss: 35.5987 - val_loss: 28.1167\n",
            "Epoch 91/1000\n",
            "10/10 - 0s - loss: 36.2141 - val_loss: 28.8022\n",
            "Epoch 92/1000\n",
            "10/10 - 0s - loss: 33.3748 - val_loss: 27.2660\n",
            "Epoch 93/1000\n",
            "10/10 - 0s - loss: 33.5639 - val_loss: 28.3910\n",
            "Epoch 94/1000\n",
            "10/10 - 0s - loss: 34.2805 - val_loss: 26.7933\n",
            "Epoch 95/1000\n",
            "10/10 - 0s - loss: 32.3179 - val_loss: 26.9823\n",
            "Epoch 96/1000\n",
            "10/10 - 0s - loss: 31.4739 - val_loss: 25.8161\n",
            "Epoch 97/1000\n",
            "10/10 - 0s - loss: 31.0765 - val_loss: 25.6851\n",
            "Epoch 98/1000\n",
            "10/10 - 0s - loss: 30.7797 - val_loss: 25.6235\n",
            "Epoch 99/1000\n",
            "10/10 - 0s - loss: 30.5942 - val_loss: 25.0483\n",
            "Epoch 100/1000\n",
            "10/10 - 0s - loss: 30.3659 - val_loss: 25.7129\n",
            "Epoch 101/1000\n",
            "10/10 - 0s - loss: 29.9138 - val_loss: 24.2207\n",
            "Epoch 102/1000\n",
            "10/10 - 0s - loss: 29.7432 - val_loss: 24.5228\n",
            "Epoch 103/1000\n",
            "10/10 - 0s - loss: 30.5435 - val_loss: 25.6571\n",
            "Epoch 104/1000\n",
            "10/10 - 0s - loss: 28.8301 - val_loss: 23.4368\n",
            "Epoch 105/1000\n",
            "10/10 - 0s - loss: 28.6457 - val_loss: 24.3242\n",
            "Epoch 106/1000\n",
            "10/10 - 0s - loss: 27.7422 - val_loss: 23.5966\n",
            "Epoch 107/1000\n",
            "10/10 - 0s - loss: 28.6301 - val_loss: 22.7055\n",
            "Epoch 108/1000\n",
            "10/10 - 0s - loss: 28.1878 - val_loss: 24.6253\n",
            "Epoch 109/1000\n",
            "10/10 - 0s - loss: 28.3627 - val_loss: 22.4619\n",
            "Epoch 110/1000\n",
            "10/10 - 0s - loss: 27.0758 - val_loss: 22.8790\n",
            "Epoch 111/1000\n",
            "10/10 - 0s - loss: 27.8603 - val_loss: 25.6323\n",
            "Epoch 112/1000\n",
            "10/10 - 0s - loss: 29.1021 - val_loss: 27.1514\n",
            "Epoch 113/1000\n",
            "10/10 - 0s - loss: 27.4764 - val_loss: 21.4345\n",
            "Epoch 114/1000\n",
            "10/10 - 0s - loss: 26.3633 - val_loss: 21.3030\n",
            "Epoch 115/1000\n",
            "10/10 - 0s - loss: 25.3920 - val_loss: 22.0270\n",
            "Epoch 116/1000\n",
            "10/10 - 0s - loss: 25.7244 - val_loss: 21.1086\n",
            "Epoch 117/1000\n",
            "10/10 - 0s - loss: 25.6728 - val_loss: 20.4965\n",
            "Epoch 118/1000\n",
            "10/10 - 0s - loss: 24.7047 - val_loss: 20.2899\n",
            "Epoch 119/1000\n",
            "10/10 - 0s - loss: 24.7793 - val_loss: 21.1234\n",
            "Epoch 120/1000\n",
            "10/10 - 0s - loss: 24.5546 - val_loss: 20.4198\n",
            "Epoch 121/1000\n",
            "10/10 - 0s - loss: 25.1941 - val_loss: 19.7663\n",
            "Epoch 122/1000\n",
            "10/10 - 0s - loss: 24.5012 - val_loss: 20.0178\n",
            "Epoch 123/1000\n",
            "10/10 - 0s - loss: 23.5663 - val_loss: 19.7516\n",
            "Epoch 124/1000\n",
            "10/10 - 0s - loss: 23.4775 - val_loss: 21.1436\n",
            "Epoch 125/1000\n",
            "10/10 - 0s - loss: 23.5680 - val_loss: 19.2682\n",
            "Epoch 126/1000\n",
            "10/10 - 0s - loss: 23.8064 - val_loss: 18.7171\n",
            "Epoch 127/1000\n",
            "10/10 - 0s - loss: 23.2047 - val_loss: 21.6185\n",
            "Epoch 128/1000\n",
            "10/10 - 0s - loss: 24.2081 - val_loss: 18.7790\n",
            "Epoch 129/1000\n",
            "10/10 - 0s - loss: 22.5558 - val_loss: 18.2556\n",
            "Epoch 130/1000\n",
            "10/10 - 0s - loss: 22.3354 - val_loss: 18.1715\n",
            "Epoch 131/1000\n",
            "10/10 - 0s - loss: 21.9711 - val_loss: 17.9026\n",
            "Epoch 132/1000\n",
            "10/10 - 0s - loss: 21.9168 - val_loss: 18.4389\n",
            "Epoch 133/1000\n",
            "10/10 - 0s - loss: 21.6987 - val_loss: 17.9639\n",
            "Epoch 134/1000\n",
            "10/10 - 0s - loss: 22.2849 - val_loss: 17.5769\n",
            "Epoch 135/1000\n",
            "10/10 - 0s - loss: 22.4582 - val_loss: 19.3167\n",
            "Epoch 136/1000\n",
            "10/10 - 0s - loss: 22.1115 - val_loss: 17.4459\n",
            "Epoch 137/1000\n",
            "10/10 - 0s - loss: 21.5206 - val_loss: 17.5918\n",
            "Epoch 138/1000\n",
            "10/10 - 0s - loss: 21.7164 - val_loss: 16.9353\n",
            "Epoch 139/1000\n",
            "10/10 - 0s - loss: 22.1782 - val_loss: 16.9606\n",
            "Epoch 140/1000\n",
            "10/10 - 0s - loss: 20.7511 - val_loss: 17.5796\n",
            "Epoch 141/1000\n",
            "10/10 - 0s - loss: 20.9828 - val_loss: 16.5043\n",
            "Epoch 142/1000\n",
            "10/10 - 0s - loss: 20.2343 - val_loss: 16.5133\n",
            "Epoch 143/1000\n",
            "10/10 - 0s - loss: 20.4879 - val_loss: 16.5885\n",
            "Epoch 144/1000\n",
            "10/10 - 0s - loss: 20.2547 - val_loss: 18.0308\n",
            "Epoch 145/1000\n",
            "10/10 - 0s - loss: 20.2962 - val_loss: 16.9510\n",
            "Epoch 146/1000\n",
            "Restoring model weights from the end of the best epoch.\n",
            "10/10 - 0s - loss: 19.7381 - val_loss: 16.5222\n",
            "Epoch 00146: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f5f0b07bd68>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9HZ2Eh8WIgV",
        "outputId": "90e0c27a-856a-4f96-b600-43c77265a7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Measure RMSE error, RMSE error is common for regression\n",
        "pred = model.predict(x_test)\n",
        "score = np.sqrt(metrics.mean_squared_error(pred, y_test))\n",
        "print(f\"Score (RMSE): {score}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Score (RMSE): 4.062550190776538\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JxTE6FbaXTs4"
      },
      "source": [
        "**Extracting Keras Weights and Manual neural network calculation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWF9lcU7XbQ_",
        "outputId": "2fcb29ee-3b0a-4869-c319-f00d3f01c9de",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Activation\n",
        "import pandas as pd\n",
        "import io\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "from sklearn import metrics\n",
        "# Create a dataset for the XOR function\n",
        "x = np.array([\n",
        "             [0,0],\n",
        "             [0,1],\n",
        "             [1,0],\n",
        "             [1,1]])\n",
        "\n",
        "y = np.array([0,1,1,0])\n",
        "\n",
        "# Build the network\n",
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, activation='relu')) \n",
        "model.add(Dense(1)) \n",
        "model.compile(loss='mean_squared_error', optimizer='adam')\n",
        "model.fit(x,y,verbose=0,epochs=10000)\n",
        "\n",
        "#Pred\n",
        "pred =  model.predict(x)\n",
        "print(pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[2.9802322e-08]\n",
            " [1.0000000e+00]\n",
            " [9.9999994e-01]\n",
            " [2.9802322e-08]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AMD7ykERUFw",
        "outputId": "183e9971-c9f3-4a87-bcff-18f13c4145af",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Dump Weights\n",
        "for layerNum, layer in enumerate(model.layers):\n",
        "  weights = layer.get_weights()[0]\n",
        "  biases = layer.get_weights()[1]\n",
        "\n",
        "  for toNeuronNum, bias in enumerate(biases):\n",
        "    print(f\"{layerNum}B -> L{layerNum+1}N{toNeuronNum}: {bias}\")\n",
        "\n",
        "  for fromNeuronNum, wgt in enumerate(weights):\n",
        "    for toNeuronNum, wgt2 in enumerate(wgt):\n",
        "      print(f\"L{layerNum}N{fromNeuronNum} -> L{layerNum+1}N{toNeuronNum} = {wgt2}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0B -> L1N0: -8.265034878718325e-09\n",
            "0B -> L1N1: 9.35364141696482e-09\n",
            "L0N0 -> L1N0 = -0.8778201937675476\n",
            "L0N0 -> L1N1 = 1.6086407899856567\n",
            "L0N1 -> L1N0 = 0.8778201937675476\n",
            "L0N1 -> L1N1 = -1.6086407899856567\n",
            "1B -> L2N0: 2.3987698938299218e-08\n",
            "L1N0 -> L2N0 = 1.1391854286193848\n",
            "L1N1 -> L2N0 = 0.6216427683830261\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MkKOmGeZQAFe"
      },
      "source": [
        "**Encoding a feature vector for keras neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ena4Jnv2QJJF",
        "outputId": "4c20af7d-2cbe-40fa-f20d-86e06989660a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"https://data.heatonresearch.com/data/t81-558/jh-simple-dataset.csv\",na_values=['NA','?'])\n",
        "display(df[0:5])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>job</th>\n",
              "      <th>area</th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "      <th>product</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>vv</td>\n",
              "      <td>c</td>\n",
              "      <td>50876.0</td>\n",
              "      <td>13.100000</td>\n",
              "      <td>1</td>\n",
              "      <td>9.017895</td>\n",
              "      <td>35</td>\n",
              "      <td>11.738935</td>\n",
              "      <td>49</td>\n",
              "      <td>0.885827</td>\n",
              "      <td>0.492126</td>\n",
              "      <td>0.071100</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>kd</td>\n",
              "      <td>c</td>\n",
              "      <td>60369.0</td>\n",
              "      <td>18.625000</td>\n",
              "      <td>2</td>\n",
              "      <td>7.766643</td>\n",
              "      <td>59</td>\n",
              "      <td>6.805396</td>\n",
              "      <td>51</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.342520</td>\n",
              "      <td>0.400809</td>\n",
              "      <td>c</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>pe</td>\n",
              "      <td>c</td>\n",
              "      <td>55126.0</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>1</td>\n",
              "      <td>3.632069</td>\n",
              "      <td>6</td>\n",
              "      <td>13.671772</td>\n",
              "      <td>44</td>\n",
              "      <td>0.944882</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.207723</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>11</td>\n",
              "      <td>c</td>\n",
              "      <td>51690.0</td>\n",
              "      <td>15.808333</td>\n",
              "      <td>1</td>\n",
              "      <td>5.372942</td>\n",
              "      <td>16</td>\n",
              "      <td>4.333286</td>\n",
              "      <td>50</td>\n",
              "      <td>0.889764</td>\n",
              "      <td>0.444882</td>\n",
              "      <td>0.361216</td>\n",
              "      <td>b</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>kl</td>\n",
              "      <td>d</td>\n",
              "      <td>28347.0</td>\n",
              "      <td>40.941667</td>\n",
              "      <td>3</td>\n",
              "      <td>3.822477</td>\n",
              "      <td>20</td>\n",
              "      <td>5.967121</td>\n",
              "      <td>38</td>\n",
              "      <td>0.744094</td>\n",
              "      <td>0.661417</td>\n",
              "      <td>0.068033</td>\n",
              "      <td>a</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id job area   income  ...  pop_dense  retail_dense     crime  product\n",
              "0   1  vv    c  50876.0  ...   0.885827      0.492126  0.071100        b\n",
              "1   2  kd    c  60369.0  ...   0.874016      0.342520  0.400809        c\n",
              "2   3  pe    c  55126.0  ...   0.944882      0.724409  0.207723        b\n",
              "3   4  11    c  51690.0  ...   0.889764      0.444882  0.361216        b\n",
              "4   5  kl    d  28347.0  ...   0.744094      0.661417  0.068033        a\n",
              "\n",
              "[5 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ReD25ns9Svah"
      },
      "source": [
        "we have categorical data like job, area, lets convert it into dummy variables, and into a original dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cshOqN_JS5cL",
        "outputId": "44e5b5c5-5288-46f4-de94-9ad7bd6ef290",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        }
      },
      "source": [
        "dummies=pd.get_dummies(df['job'], prefix=\"job\")\n",
        "print(dummies.shape)\n",
        "display(dummies[0:5])"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 33)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>job_11</th>\n",
              "      <th>job_al</th>\n",
              "      <th>job_am</th>\n",
              "      <th>job_ax</th>\n",
              "      <th>job_bf</th>\n",
              "      <th>job_by</th>\n",
              "      <th>job_cv</th>\n",
              "      <th>job_de</th>\n",
              "      <th>job_dz</th>\n",
              "      <th>job_e2</th>\n",
              "      <th>job_f8</th>\n",
              "      <th>job_gj</th>\n",
              "      <th>job_gv</th>\n",
              "      <th>job_kd</th>\n",
              "      <th>job_ke</th>\n",
              "      <th>job_kl</th>\n",
              "      <th>job_kp</th>\n",
              "      <th>job_ks</th>\n",
              "      <th>job_kw</th>\n",
              "      <th>job_mm</th>\n",
              "      <th>job_nb</th>\n",
              "      <th>job_nn</th>\n",
              "      <th>job_ob</th>\n",
              "      <th>job_pe</th>\n",
              "      <th>job_po</th>\n",
              "      <th>job_pq</th>\n",
              "      <th>job_pz</th>\n",
              "      <th>job_qp</th>\n",
              "      <th>job_qw</th>\n",
              "      <th>job_rn</th>\n",
              "      <th>job_sa</th>\n",
              "      <th>job_vv</th>\n",
              "      <th>job_zz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   job_11  job_al  job_am  job_ax  ...  job_rn  job_sa  job_vv  job_zz\n",
              "0       0       0       0       0  ...       0       0       1       0\n",
              "1       0       0       0       0  ...       0       0       0       0\n",
              "2       0       0       0       0  ...       0       0       0       0\n",
              "3       1       0       0       0  ...       0       0       0       0\n",
              "4       0       0       0       0  ...       0       0       0       0\n",
              "\n",
              "[5 rows x 33 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d-Oh_te_TN3R",
        "outputId": "0160a3df-6e5e-41ad-b262-0a3b1e8b7f10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "df = pd.concat([df,dummies],axis=1)\n",
        "df.drop('job', axis=1, inplace=True)\n",
        "display(df[0:10])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>area</th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "      <th>product</th>\n",
              "      <th>job_11</th>\n",
              "      <th>job_al</th>\n",
              "      <th>job_am</th>\n",
              "      <th>job_ax</th>\n",
              "      <th>job_bf</th>\n",
              "      <th>job_by</th>\n",
              "      <th>job_cv</th>\n",
              "      <th>job_de</th>\n",
              "      <th>job_dz</th>\n",
              "      <th>job_e2</th>\n",
              "      <th>job_f8</th>\n",
              "      <th>job_gj</th>\n",
              "      <th>job_gv</th>\n",
              "      <th>job_kd</th>\n",
              "      <th>job_ke</th>\n",
              "      <th>job_kl</th>\n",
              "      <th>job_kp</th>\n",
              "      <th>job_ks</th>\n",
              "      <th>job_kw</th>\n",
              "      <th>job_mm</th>\n",
              "      <th>job_nb</th>\n",
              "      <th>job_nn</th>\n",
              "      <th>job_ob</th>\n",
              "      <th>job_pe</th>\n",
              "      <th>job_po</th>\n",
              "      <th>job_pq</th>\n",
              "      <th>job_pz</th>\n",
              "      <th>job_qp</th>\n",
              "      <th>job_qw</th>\n",
              "      <th>job_rn</th>\n",
              "      <th>job_sa</th>\n",
              "      <th>job_vv</th>\n",
              "      <th>job_zz</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>c</td>\n",
              "      <td>50876.0</td>\n",
              "      <td>13.100000</td>\n",
              "      <td>1</td>\n",
              "      <td>9.017895</td>\n",
              "      <td>35</td>\n",
              "      <td>11.738935</td>\n",
              "      <td>49</td>\n",
              "      <td>0.885827</td>\n",
              "      <td>0.492126</td>\n",
              "      <td>0.071100</td>\n",
              "      <td>b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>c</td>\n",
              "      <td>60369.0</td>\n",
              "      <td>18.625000</td>\n",
              "      <td>2</td>\n",
              "      <td>7.766643</td>\n",
              "      <td>59</td>\n",
              "      <td>6.805396</td>\n",
              "      <td>51</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.342520</td>\n",
              "      <td>0.400809</td>\n",
              "      <td>c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>c</td>\n",
              "      <td>55126.0</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>1</td>\n",
              "      <td>3.632069</td>\n",
              "      <td>6</td>\n",
              "      <td>13.671772</td>\n",
              "      <td>44</td>\n",
              "      <td>0.944882</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.207723</td>\n",
              "      <td>b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>c</td>\n",
              "      <td>51690.0</td>\n",
              "      <td>15.808333</td>\n",
              "      <td>1</td>\n",
              "      <td>5.372942</td>\n",
              "      <td>16</td>\n",
              "      <td>4.333286</td>\n",
              "      <td>50</td>\n",
              "      <td>0.889764</td>\n",
              "      <td>0.444882</td>\n",
              "      <td>0.361216</td>\n",
              "      <td>b</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>d</td>\n",
              "      <td>28347.0</td>\n",
              "      <td>40.941667</td>\n",
              "      <td>3</td>\n",
              "      <td>3.822477</td>\n",
              "      <td>20</td>\n",
              "      <td>5.967121</td>\n",
              "      <td>38</td>\n",
              "      <td>0.744094</td>\n",
              "      <td>0.661417</td>\n",
              "      <td>0.068033</td>\n",
              "      <td>a</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>c</td>\n",
              "      <td>70854.0</td>\n",
              "      <td>40.400000</td>\n",
              "      <td>1</td>\n",
              "      <td>14.893343</td>\n",
              "      <td>87</td>\n",
              "      <td>20.340593</td>\n",
              "      <td>43</td>\n",
              "      <td>0.866142</td>\n",
              "      <td>0.673228</td>\n",
              "      <td>0.473581</td>\n",
              "      <td>d</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>d</td>\n",
              "      <td>38726.0</td>\n",
              "      <td>30.975000</td>\n",
              "      <td>3</td>\n",
              "      <td>3.822477</td>\n",
              "      <td>33</td>\n",
              "      <td>9.480399</td>\n",
              "      <td>39</td>\n",
              "      <td>0.976378</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.092151</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>a</td>\n",
              "      <td>55162.0</td>\n",
              "      <td>26.966667</td>\n",
              "      <td>2</td>\n",
              "      <td>4.312097</td>\n",
              "      <td>17</td>\n",
              "      <td>29.219896</td>\n",
              "      <td>44</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.162833</td>\n",
              "      <td>b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>c</td>\n",
              "      <td>67311.0</td>\n",
              "      <td>32.383333</td>\n",
              "      <td>0</td>\n",
              "      <td>25.093772</td>\n",
              "      <td>169</td>\n",
              "      <td>10.927357</td>\n",
              "      <td>45</td>\n",
              "      <td>0.952756</td>\n",
              "      <td>0.681102</td>\n",
              "      <td>0.096333</td>\n",
              "      <td>c</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>a</td>\n",
              "      <td>63344.0</td>\n",
              "      <td>38.233333</td>\n",
              "      <td>1</td>\n",
              "      <td>2.816034</td>\n",
              "      <td>3</td>\n",
              "      <td>21.915695</td>\n",
              "      <td>42</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.173986</td>\n",
              "      <td>c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id area   income     aspect  ...  job_rn  job_sa  job_vv  job_zz\n",
              "0   1    c  50876.0  13.100000  ...       0       0       1       0\n",
              "1   2    c  60369.0  18.625000  ...       0       0       0       0\n",
              "2   3    c  55126.0  34.766667  ...       0       0       0       0\n",
              "3   4    c  51690.0  15.808333  ...       0       0       0       0\n",
              "4   5    d  28347.0  40.941667  ...       0       0       0       0\n",
              "5   6    c  70854.0  40.400000  ...       0       0       0       0\n",
              "6   7    d  38726.0  30.975000  ...       0       0       0       0\n",
              "7   8    a  55162.0  26.966667  ...       0       0       0       0\n",
              "8   9    c  67311.0  32.383333  ...       0       0       0       0\n",
              "9  10    a  63344.0  38.233333  ...       0       0       0       0\n",
              "\n",
              "[10 rows x 46 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_dQ69GjnTtFI"
      },
      "source": [
        "**We also need dummy variables for area column**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5u4TI5ZzTyg6",
        "outputId": "6b8d5965-0446-4af5-e55c-8ed2fd4f39a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379
        }
      },
      "source": [
        "df = pd.concat([df,pd.get_dummies(df['area'],prefix=\"area\")],axis=1)\n",
        "df.drop('area',axis=1,inplace=True)\n",
        "display(df[0:10])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>income</th>\n",
              "      <th>aspect</th>\n",
              "      <th>subscriptions</th>\n",
              "      <th>dist_healthy</th>\n",
              "      <th>save_rate</th>\n",
              "      <th>dist_unhealthy</th>\n",
              "      <th>age</th>\n",
              "      <th>pop_dense</th>\n",
              "      <th>retail_dense</th>\n",
              "      <th>crime</th>\n",
              "      <th>product</th>\n",
              "      <th>job_11</th>\n",
              "      <th>job_al</th>\n",
              "      <th>job_am</th>\n",
              "      <th>job_ax</th>\n",
              "      <th>job_bf</th>\n",
              "      <th>job_by</th>\n",
              "      <th>job_cv</th>\n",
              "      <th>job_de</th>\n",
              "      <th>job_dz</th>\n",
              "      <th>job_e2</th>\n",
              "      <th>job_f8</th>\n",
              "      <th>job_gj</th>\n",
              "      <th>job_gv</th>\n",
              "      <th>job_kd</th>\n",
              "      <th>job_ke</th>\n",
              "      <th>job_kl</th>\n",
              "      <th>job_kp</th>\n",
              "      <th>job_ks</th>\n",
              "      <th>job_kw</th>\n",
              "      <th>job_mm</th>\n",
              "      <th>job_nb</th>\n",
              "      <th>job_nn</th>\n",
              "      <th>job_ob</th>\n",
              "      <th>job_pe</th>\n",
              "      <th>job_po</th>\n",
              "      <th>job_pq</th>\n",
              "      <th>job_pz</th>\n",
              "      <th>job_qp</th>\n",
              "      <th>job_qw</th>\n",
              "      <th>job_rn</th>\n",
              "      <th>job_sa</th>\n",
              "      <th>job_vv</th>\n",
              "      <th>job_zz</th>\n",
              "      <th>area_a</th>\n",
              "      <th>area_b</th>\n",
              "      <th>area_c</th>\n",
              "      <th>area_d</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>50876.0</td>\n",
              "      <td>13.100000</td>\n",
              "      <td>1</td>\n",
              "      <td>9.017895</td>\n",
              "      <td>35</td>\n",
              "      <td>11.738935</td>\n",
              "      <td>49</td>\n",
              "      <td>0.885827</td>\n",
              "      <td>0.492126</td>\n",
              "      <td>0.071100</td>\n",
              "      <td>b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>60369.0</td>\n",
              "      <td>18.625000</td>\n",
              "      <td>2</td>\n",
              "      <td>7.766643</td>\n",
              "      <td>59</td>\n",
              "      <td>6.805396</td>\n",
              "      <td>51</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.342520</td>\n",
              "      <td>0.400809</td>\n",
              "      <td>c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>55126.0</td>\n",
              "      <td>34.766667</td>\n",
              "      <td>1</td>\n",
              "      <td>3.632069</td>\n",
              "      <td>6</td>\n",
              "      <td>13.671772</td>\n",
              "      <td>44</td>\n",
              "      <td>0.944882</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.207723</td>\n",
              "      <td>b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>51690.0</td>\n",
              "      <td>15.808333</td>\n",
              "      <td>1</td>\n",
              "      <td>5.372942</td>\n",
              "      <td>16</td>\n",
              "      <td>4.333286</td>\n",
              "      <td>50</td>\n",
              "      <td>0.889764</td>\n",
              "      <td>0.444882</td>\n",
              "      <td>0.361216</td>\n",
              "      <td>b</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>28347.0</td>\n",
              "      <td>40.941667</td>\n",
              "      <td>3</td>\n",
              "      <td>3.822477</td>\n",
              "      <td>20</td>\n",
              "      <td>5.967121</td>\n",
              "      <td>38</td>\n",
              "      <td>0.744094</td>\n",
              "      <td>0.661417</td>\n",
              "      <td>0.068033</td>\n",
              "      <td>a</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>6</td>\n",
              "      <td>70854.0</td>\n",
              "      <td>40.400000</td>\n",
              "      <td>1</td>\n",
              "      <td>14.893343</td>\n",
              "      <td>87</td>\n",
              "      <td>20.340593</td>\n",
              "      <td>43</td>\n",
              "      <td>0.866142</td>\n",
              "      <td>0.673228</td>\n",
              "      <td>0.473581</td>\n",
              "      <td>d</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>7</td>\n",
              "      <td>38726.0</td>\n",
              "      <td>30.975000</td>\n",
              "      <td>3</td>\n",
              "      <td>3.822477</td>\n",
              "      <td>33</td>\n",
              "      <td>9.480399</td>\n",
              "      <td>39</td>\n",
              "      <td>0.976378</td>\n",
              "      <td>0.874016</td>\n",
              "      <td>0.092151</td>\n",
              "      <td>f</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>8</td>\n",
              "      <td>55162.0</td>\n",
              "      <td>26.966667</td>\n",
              "      <td>2</td>\n",
              "      <td>4.312097</td>\n",
              "      <td>17</td>\n",
              "      <td>29.219896</td>\n",
              "      <td>44</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.162833</td>\n",
              "      <td>b</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>9</td>\n",
              "      <td>67311.0</td>\n",
              "      <td>32.383333</td>\n",
              "      <td>0</td>\n",
              "      <td>25.093772</td>\n",
              "      <td>169</td>\n",
              "      <td>10.927357</td>\n",
              "      <td>45</td>\n",
              "      <td>0.952756</td>\n",
              "      <td>0.681102</td>\n",
              "      <td>0.096333</td>\n",
              "      <td>c</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>10</td>\n",
              "      <td>63344.0</td>\n",
              "      <td>38.233333</td>\n",
              "      <td>1</td>\n",
              "      <td>2.816034</td>\n",
              "      <td>3</td>\n",
              "      <td>21.915695</td>\n",
              "      <td>42</td>\n",
              "      <td>0.897638</td>\n",
              "      <td>0.724409</td>\n",
              "      <td>0.173986</td>\n",
              "      <td>c</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id   income     aspect  subscriptions  ...  area_a  area_b  area_c  area_d\n",
              "0   1  50876.0  13.100000              1  ...       0       0       1       0\n",
              "1   2  60369.0  18.625000              2  ...       0       0       1       0\n",
              "2   3  55126.0  34.766667              1  ...       0       0       1       0\n",
              "3   4  51690.0  15.808333              1  ...       0       0       1       0\n",
              "4   5  28347.0  40.941667              3  ...       0       0       0       1\n",
              "5   6  70854.0  40.400000              1  ...       0       0       1       0\n",
              "6   7  38726.0  30.975000              3  ...       0       0       0       1\n",
              "7   8  55162.0  26.966667              2  ...       1       0       0       0\n",
              "8   9  67311.0  32.383333              0  ...       0       0       1       0\n",
              "9  10  63344.0  38.233333              1  ...       1       0       0       0\n",
              "\n",
              "[10 rows x 49 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRGmIiyyWMIw",
        "outputId": "8177d37d-bafe-4ee8-dbb4-5505ef21cba4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "#find missing values\n",
        "print(df['income'].isnull().values.any())"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4u5Y7J24WiPu"
      },
      "source": [
        "med = df['income'].median()\n",
        "df['income'] = df['income'].fillna(med)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DW-4JkxJWv6n",
        "outputId": "761ff258-3f95-4800-9ccc-806906fe3e45",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "print(list(df.columns))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['id', 'income', 'aspect', 'subscriptions', 'dist_healthy', 'save_rate', 'dist_unhealthy', 'age', 'pop_dense', 'retail_dense', 'crime', 'product', 'job_11', 'job_al', 'job_am', 'job_ax', 'job_bf', 'job_by', 'job_cv', 'job_de', 'job_dz', 'job_e2', 'job_f8', 'job_gj', 'job_gv', 'job_kd', 'job_ke', 'job_kl', 'job_kp', 'job_ks', 'job_kw', 'job_mm', 'job_nb', 'job_nn', 'job_ob', 'job_pe', 'job_po', 'job_pq', 'job_pz', 'job_qp', 'job_qw', 'job_rn', 'job_sa', 'job_vv', 'job_zz', 'area_a', 'area_b', 'area_c', 'area_d']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhMlKexKe_O2",
        "outputId": "602ee74f-d9e5-4f3e-9cc9-028b02992931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "x_columns = df.columns.drop('product').drop('id')\n",
        "print(list(x_columns))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['income', 'aspect', 'subscriptions', 'dist_healthy', 'save_rate', 'dist_unhealthy', 'age', 'pop_dense', 'retail_dense', 'crime', 'job_11', 'job_al', 'job_am', 'job_ax', 'job_bf', 'job_by', 'job_cv', 'job_de', 'job_dz', 'job_e2', 'job_f8', 'job_gj', 'job_gv', 'job_kd', 'job_ke', 'job_kl', 'job_kp', 'job_ks', 'job_kw', 'job_mm', 'job_nb', 'job_nn', 'job_ob', 'job_pe', 'job_po', 'job_pq', 'job_pz', 'job_qp', 'job_qw', 'job_rn', 'job_sa', 'job_vv', 'job_zz', 'area_a', 'area_b', 'area_c', 'area_d']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHf4b_REfoXF"
      },
      "source": [
        "**Generate X and Y for a Classification Neural Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "juVTjg-yfzvR"
      },
      "source": [
        "# Convert to Numpy Classification\n",
        "x_columns = df.columns.drop('product').drop('id')\n",
        "x = df[x_columns].values\n",
        "dummies = pd.get_dummies(df['product']) # classification\n",
        "products = dummies.columns\n",
        "y = dummies.values"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DXFnLlvagngh",
        "outputId": "afe6fa26-25cf-41ce-c264-4ccfe0cef557",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "print(x)\n",
        "print(y)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[5.08760000e+04 1.31000000e+01 1.00000000e+00 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [6.03690000e+04 1.86250000e+01 2.00000000e+00 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [5.51260000e+04 3.47666667e+01 1.00000000e+00 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " ...\n",
            " [2.85950000e+04 3.94250000e+01 3.00000000e+00 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.00000000e+00]\n",
            " [6.79490000e+04 5.73333333e+00 0.00000000e+00 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]\n",
            " [6.14670000e+04 1.68916667e+01 0.00000000e+00 ... 0.00000000e+00\n",
            "  1.00000000e+00 0.00000000e+00]]\n",
            "[[0 1 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 1 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 1 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f01OTbSMhqBa"
      },
      "source": [
        ">Classification neural network have an output neuron count equal to no of classes.\n",
        ">Classification neural network should use **categorical_crossentropy** and a **softmax** activation function on the output layer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piSo58dzkCq_"
      },
      "source": [
        "**Generate X and Y for regression neural network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kc7NpDoiJey"
      },
      "source": [
        "# if we have missing values in target columns, its better to drop these rows, \n",
        "# we won't want to train the neural network on median values.\n",
        "y = df['income'].values"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}